 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ1: What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)?
Answer: The provided context does not explicitly mention the methods utilized for collecting raw data in the deep learning pipeline. However, it does mention the use of satellite-based observations for establishing the probability distribution of Rhododendron arboreum. Therefore, it can be inferred that satellite sensors are used for collecting raw data in this study.

CQ2: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?
Answer: The context does not provide information on the specific data formats used in the deep learning pipeline. However, it mentions that the Convolutional Neural Network (CNN) is used for processing multidimensional data that follows a grid pattern. This suggests that the data format could be an image or a similar grid-like data structure.

CQ3: What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?
Answer: The provided context does not mention any specific data annotation techniques used in the deep learning pipeline. Therefore, I cannot provide an answer to this query based on the given context.

CQ4: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?
Answer: The context does not provide information on the data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide an answer to this query.

CQ5: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)?
Answer: The context does not provide information on the specific datasets used in the deep learning pipeline.

CQ6: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide information on the preprocessing steps involved before training a deep learning model. However, common preprocessing steps include normalization, scaling, and cleaning the data. Normalization is the process of rescaling the data to a specific range, often between 0 and 1. Scaling is the process of adjusting the range of the data to a specific scale, often between -1 and 1. Cleaning the data involves removing any missing or irrelevant data points. %Query

CQ7: What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?
Answer: The context does not provide information on the criteria used to split the data for deep learning model training. However, it is common practice to split the data into training, validation, and testing sets. The training set is used to train the model, the validation set is used to tune hyperparameters and prevent overfitting, and the testing set is used to evaluate the final performance of the model. The split is often done randomly, but it can also be done in a stratified manner to ensure that each set has a similar distribution of classes. %Query

CQ8: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?
Answer: The code repository of the deep learning pipeline is not mentioned in the provided context.

CQ9: Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?
Answer: The article does not provide information on the data repository of the deep learning pipeline.

CQ10: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, but the provided context does not include information about the code repository link of the deep learning pipeline.

CQ11: What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?
Answer: I'm sorry, but the provided context does not include information about the data repository link of the deep learning pipeline.

CQ12: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a Convolutional Neural Network (CNN) as the deep learning model.

CQ13: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The provided context does not explicitly mention the hyperparameters used in the deep learning model, such as learning rate or optimizer. However, it does mention that the hyperparameters involved in convolution operations are the size and number of kernels. The size could be anything from 3 × 3 to 5 × 5 to 7 × 7, and the kernel could be chosen randomly.

CQ14: How are the hyperparameters of the model optimized (e.g., grid search, random search)?
Answer: The performance evaluation of the model is conducted by tuning the learnable parameters, kernels, and weight by a loss function through the forward propagation followed by updating these parameter values through an optimization algorithm either by backpropagation or gradient descent. However, the context does not provide specific information on how the hyperparameters of the model are optimized, such as using grid search or random search.

CQ15: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The performance evaluation of the model is conducted by tuning the learnable parameters, kernels, and weight by a loss function through the forward propagation followed by updating these parameter values through an optimization algorithm either by backpropagation or gradient descent. The answer does not explicitly mention the optimization techniques applied in the deep learning pipeline, but it does mention that the parameter values are updated through an optimization algorithm, either by backpropagation or gradient descent. However, it does not specify which optimization techniques, such as SGD or Adam, are used. Therefore, the answer is not fully helpful in this context. Final

CQ16: What criteria are used to determine when training is complete (e.g., validation loss plateau)?
Answer: The performance evaluation of the model is conducted by tuning the learnable parameters, kernels, and weight by a loss function through the forward propagation followed by updating these parameter values through an optimization algorithm either by backpropagation or gradient descent. In any type of modeling, performance evaluation is an essential task. In terms of validation of species probability distribution, the AUC (Area Under ROC (Receiver Operating Characteristics) Curve) is one of the most used performance evaluation metrics. The primary application of the ROC curve is in the threshold independent assessment that characterizes the model performance at various discrimination thresholds. This application was found in raster-based studies focusing on predicting land use and land cover, species distribution modeling, risk assessment, and other probability mappings. The AUC is generated by plotting the True Positive Rate (TPR) versus the False Positive Rate (FPR) at varied thresholds. The TPR is also known as sensitivity, probability of detection, or recall, and the FPR is also known as the probability of false alarm. Therefore, an accurate model will generate a ROC curve away from the 1:1 line, and a less accurate model will have a ROC curve towards the 1:1 line. The range of the AUC varies from 0 to 1. The closer the value is to 1, the better the prediction is. The plots can be described mathematically as: TPR or Sensitivity or Recall or Probability of Detection = TP TP + FN × 100 Specificity = TN TN + FP × 100 FPR or Probability of false alarm = 1 − Specificity Here, TP stands for true positive, and FP is false positive, where specificity is also termed the true negative rate. The TPR provides the percentage of correctly predicted instances of species other than rhododendron, whereas specificity provides the percentage of correctly predicted instances of rhododendron distribution.

CQ17: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The provided context does not mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods used in deep learning include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing the model's reliance on any single neuron. L2 regularization, also known as weight decay, adds a penalty term to the loss function that discourages large weights, which can help to prevent overfitting by reducing the model's complexity.

CQ18: What is the strategy implemented to monitor the model performance during training?
Answer: The strategy implemented to monitor the model performance during training is tuning the learnable parameters, kernels, and weight by a loss function through the forward propagation followed by updating these parameter values through an optimization algorithm either by backpropagation or gradient descent.

CQ19: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The context does not provide information on the specific frameworks used to build the deep learning model.

CQ20: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context does not provide information on the hardware resources used for training the deep learning model.

CQ21: What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?
Answer: After the model training, the postprocessing steps involved are the performance evaluation of the model, which is conducted by tuning the learnable parameters, kernels, and weight by a loss function through the forward propagation followed by updating these parameter values through an optimization algorithm either by backpropagation or gradient descent. In terms of validation of species probability distribution, the AUC (Area Under ROC (Receiver Operating Characteristics) Curve) is one of the most used performance evaluation metrics. The primary application of the ROC curve is in the threshold independent assessment that characterizes the model performance at various discrimination thresholds. The AUC is generated by plotting the True Positive Rate (TPR) versus the False Positive Rate (FPR) at varied thresholds. The TPR is also known as sensitivity, probability of detection, or recall, and the FPR is also known as the probability of false alarm. Therefore, an accurate model will generate a ROC curve away from the 1:1 line, and a less accurate model will have a ROC curve towards the 1:1 line. The range of the AUC varies from 0 to 1. The closer the value is to 1, the better the prediction is. The plots can be described mathematically as: TPR or Sensitivity or Recall or Probability of Detection = TP TP + FN × 100 Specificity = TN TN + FP × 100 FPR or Probability of false alarm = 1 − Specificity Here, TP stands for true positive, and FP is false positive, where specificity is also termed the true negative rate. The TPR provides the percentage of correctly predicted instances of species other than rhododendron, whereas specificity provides the percentage of correctly predicted instances of rhododendron distribution. Additionally, a correlation matrix plot was drawn to understand the relationship between the input parameters. Total fifteen parameters except for the biome layer, which is in the vector form, were used in the correlation matrix. A highly linear or nonlinear relationship shows a relation/dependency or non- relation/non-dependency between the parameters. The representation can be explained in terms of the values varying from −1 to +1. The −ve value represents the negative relationship, and the +ve value describes the positive relationship. The depiction of the negative relationship is in orange, where the higher correlation value is visualized through the steeper circular shape, and vice versa for the positive relationship. No relationship is represented by the correlation value of zero that is represented by a perfect circular shape, and the colour becomes whitish. It can be observed that many parameters are related to each other. A highly linear relationship exists between Sentinel-5p based ozone and carbon monoxide as well as with ozone and water vapour with a correlation value of 0. 99. It can also be observed that the linear relationship of DEM with NDVI, LST, and water vapour is very high, which shows the variation in the local geometry and the influence of regional ecological and climatic parameters. A lower correlation value is observed between the atmospheric parameters and the vegetation indices, especially for EVI, LAI, and ET. Furthermore, the spatial distribution of Rhododendron arboreum was also analyzed. Therefore, the postprocessing steps involved after the model training are the performance evaluation of the model using metrics such as the AUC (Area Under ROC (Receiver Operating Characteristics) Curve), analyzing the relationship between the input parameters using a correlation matrix plot, and analyzing the spatial distribution of Rhododendron arboreum.

CQ22: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance evaluation of the deep learning model is conducted using the Area Under ROC (Receiver Operating Characteristics) Curve (AUC) and Cohen's kappa. The AUC is generated by plotting the True Positive Rate (TPR) versus the False Positive Rate (FPR) at varied thresholds. The TPR is also known as sensitivity, probability of detection, or recall, and the FPR is also known as the probability of false alarm. An accurate model will generate a ROC curve away from the 1:1 line, and a less accurate model will have a ROC curve towards the 1:1 line. The range of the AUC varies from 0 to 1. The closer the value is to 1, the better the prediction is. Cohen’s kappa is also calculated to support the AUC value. Being one of the most popular performance evaluation indices, it is considered to be less complex and dependent on prevalence. The kappa value ranges from −1 to +1, where +1 indicates the perfect agreement. Other than kappa, the True Skill Statistic (TSS) is also incorporated, as it corrects the unimodel dependency of kappa. TSS is widely used in ecology, and it can be explained as TSS = Sensitivity − Speciﬁcity − 1.

CQ23: What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?
Answer: The context does not provide information on the specific measures taken to ensure the generalizability of the deep learning model. However, it does mention that the model was validated using 30% of the data, which could be a form of cross-validation. %Query

CQ24: What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?
Answer: The strategies employed to handle randomness in the deep learning pipeline, such as the random seed value, include setting a fixed seed value for reproducibility, using techniques like cross- validation, and employing ensemble methods to reduce the impact of randomness on the final results. %Context Context: Deep learning models are often trained using stochastic optimization algorithms, which introduce randomness into the training process. This randomness can affect the performance of the model and make it difficult to reproduce results. To handle this, several strategies are employed: 1. Setting a fixed seed value: By setting a fixed seed value for the random number generator, the same sequence of random numbers will be generated each time the code is run. This can help ensure reproducibility of results. 2. Cross-validation: Cross-validation is a technique used to evaluate the performance of a model by training it on different subsets of the data and testing it on the remaining data. This can help reduce the impact of randomness on the final results. 3. Ensemble methods: Ensemble methods combine the predictions of multiple models to make a final prediction. By using an ensemble of models, the impact of randomness on the final results can be reduced. %Answer

CQ25: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model in this study is to use the Convolutional Neural Network (CNN) architecture for Species Distribution Modelling (SDM). The model is developed in such a way that the algorithm learns and adapts to the spatial hierarchies of features by itself from the lower to the higher levels of the pattern. The model is composed of three layers or building blocks: convolution, pooling, and fully connected layers. Feature extraction is conducted using the first two layers and mapping the extracted features to the output is conducted by the third layer. The model is used to track and map the distribution of the Rhododendron arboreum species based on its occurrence and several ecological and bioclimatic satellite-based observations. The CNN based probability distribution model outperformed the presence only based BIOCLIM model with an AUC score of 0. 917.

CQ26: What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?
Answer: The provided context does not mention any techniques used to address data bias during preprocessing of the deep learning pipeline. However, some common techniques used to address data bias include stratified splitting, oversampling, undersampling, and diverse data collection. Stratified splitting involves splitting the data into training and testing sets while maintaining the same distribution of classes in both sets. Oversampling involves increasing the number of instances in the minority class, while undersampling involves decreasing the number of instances in the majority class. Diverse data collection involves collecting data from a variety of sources to ensure that the model is trained on a diverse set of data.

CQ27: What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?
Answer: The provided context does not contain information about the process followed to deploy the trained deep learning model, such as model serialization or platform selection. Therefore, I cannot provide an answer to this query based on the given context.

CQ28: Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?
Answer: The context does not provide information on the platform used to deploy the deep learning model.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ... 