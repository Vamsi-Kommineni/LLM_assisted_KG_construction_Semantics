 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ1: What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)?
Answer: The raw data in the deep learning pipeline is collected using video clips taken by camera traps positioned at an "animals' bridge" (wildlife crossing) that is crossing the federal motorway 7 near the city Oberthulba. The camera traps are equipped with infrared camera and infrared flash lights. %INSTRUCTIONS: Use the provided pieces of context to answer the query. If you don't know the answer, just say that you don't know, don't try to make up an answer. %Query

CQ2: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?
Answer: The deep learning pipeline uses video data formats. The video clips are captured with 8 frames per second by camera traps using infrared cameras and infrared flash-lights. The clips show deer, boars, foxes, and hares, mostly at night time.

CQ3: What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?
Answer: The VGG Image Annotator (VIA) Version 2. 0. 8 (Dutta and Zisserman (2019)) is used for the annotation of the segmentation dataset. For each occurring animal in a frame of the training set, a polygonal segmentation mask depicting the visible silhouette, a bounding box and a class label, i. e. , deer, boar, fox, hare, are annotated. The polygonal silhouette of a deer shows typically around 70 vertices, while the polygonal silhouette of a hare may show only around 35 vertices.

CQ4: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?
Answer: The data augmentation techniques applied in the deep learning pipeline are horizontal flip, rotation, scaling, and shearing. These techniques are applied using the python library imgaug (Jung et al. (2020)) for the data augmentation of images and videos. For Mask R-CNN and FGFA, the same augmentation technique with identical parameters is applied to each frame of the video.

CQ5: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)?
Answer: The datasets used in the deep learning pipeline are not explicitly mentioned in the provided context. However, the context does mention the use of a dataset for animal action recognition, which is a subset of the dataset AnnotationsActions. This dataset is not a standard dataset like MNIST, CIFAR, or ImageNet.

CQ6: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide information on the preprocessing steps involved before training a deep learning model. However, common preprocessing steps for deep learning models include normalization, scaling, and cleaning. Normalization involves scaling the input features to a range between 0 and 1 or -1 and 1. Scaling involves resizing the input features to a fixed size. Cleaning involves removing or correcting any errors or inconsistencies in the data.

CQ7: What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?
Answer: The annotated dataset AnnotationsActions is split randomly (but fixed for all experiments and all networks) into a train set, validation set, and test set. The test set includes 20% of the video clips of each class. Of the remaining clips, again 20% of the clips in each class form the validation set. The remaining clips are the training set.

CQ8: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?
Answer: The code repository of the deep learning pipeline is not mentioned in the provided context. Therefore, I don't know where it is available.

CQ9: Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?
Answer: The data repository of the deep learning pipeline is not mentioned in the provided context. Therefore, I don't know where it is available.

CQ10: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, but the provided context does not include the code repository link of the deep learning pipeline.

CQ11: What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?
Answer: I'm sorry, but the provided context does not contain information about the data repository link of the deep learning pipeline.

CQ12: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a SlowFast architecture, which is a type of deep learning model that uses two pathways for video input, a Slow pathway and a Fast pathway. The Slow pathway works on a low frame rate and low temporal resolution for detecting spatial semantics, while the Fast pathway uses a high frame rate to capture motion and the temporal aspects in the video. The backbone for the pathways can be implemented by different networks, with ResNets achieving the best results. SlowFast is a single-stream architecture that works on two different frame rates.

CQ13: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The deep learning model uses Stochastic Gradient Descent (SGD) as the optimization technique. The initial learning rate is 0. 001, and it is decreased every 10 epochs by multiplying it with γ = 0. 1. The momentum is set to 0. 9, and the weight decay is 5 ⋅ 10. The loss function used is the cross entropy loss. The model is trained for 40 epochs.

CQ14: How are the hyperparameters of the model optimized (e.g., grid search, random search)?
Answer: The hyperparameters of the model are optimized by checking different settings of momentum and weight decay, but no improvements are found. The learning rate is decreased every 10 epochs by multiplying it with 0. 1. The model is trained for 30 epochs. The parameters that gave the best results in the evaluations are used.

CQ15: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The optimization technique used in the deep learning pipeline is Stochastic Gradient Descent (SGD). The learning rate is initially set to 0. 0005 and is decreased every 10 epochs by multiplying it with 0. 1. In the first epoch, warmup iterations are used for the learning rate with a warmup factor of 1/1000. The momentum is set to 0. 9 and the weight decay is 0. 0005. These parameters gave the best results in the evaluations. The models are trained for 30 epochs.

CQ16: What criteria are used to determine when training is complete (e.g., validation loss plateau)?
Answer: The training process is completed after 30 epochs for both networks. The evaluation results for frame sequences showing animals with less articulated characteristics are depicted in the right columns of Table 4. Both networks show similar results. Especially FGFA achieves much higher precision and recall values than for the basic evaluation set. Also the values for Mask R-CNN increase on this test set. FGFA is still slightly worse than Mask R-CNN. The training process is not based on a validation loss plateau.

CQ17: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The paper mentions the use of data augmentation techniques to prevent overfitting and generalize better. Specifically, the python library imgaug is used for data augmentation of images and videos. The augmentation techniques used include horizontal flip. However, the paper does not explicitly mention the use of regularization methods such as dropout or L2 regularization.

CQ18: What is the strategy implemented to monitor the model performance during training?
Answer: The strategy implemented to monitor the model performance during training is to use a validation set. The validation set is used to evaluate the model performance after each epoch. The best model is selected based on the performance on the validation set. %Context Context: We split the annotated dataset AnnotationsActions (cf. Section 2. 2) randomly (but fixed for all experiments and all networks) into a train set, validation set and test set. The test set includes 20% of the video clips of each class. Of the remaining clips, again 20% of the clips in each class form the validation set. The remaining clips are the training set. %Query

CQ19: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The query does not provide enough context to determine which frameworks are used to build the deep learning model.

CQ20: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The training and testing were performed with a GeForce RTX 2080 Ti GPU with 11 GB graphic memory, 16 GB RAM, and an Intel Core i7-6700K 4. 00 GHz CPU.

CQ21: What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?
Answer: The postprocessing steps involved after the model training are not explicitly mentioned in the provided context. However, it is mentioned that the evaluation metrics used are the official COCO evaluation metrics, following Lin et al. (2014) and Everingham et al. (2010). These metrics include the calculation of the Intersection over Union (IoU) for bounding boxes and segmentation masks. Additionally, the use of confidence scores for each bounding box or segmentation mask prediction is mentioned. It is also stated that the evaluation results are depicted in tables and figures, which suggests that metrics calculation is a postprocessing step. However, the context does not provide information on the use of saliency maps or confusion matrix.

CQ22: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The evaluation metrics used to evaluate the performance of the deep learning model are Average Precision (AP) and Average Recall (AR). AP is a numeric metric to compare precision-recall curves from different detectors, while AR is averaged over different IoU thresholds. The AP for COCO datasets is calculated as the average precision for different recall levels of the same detector, while the AR is calculated as the average recall for different IoU thresholds. The AR is also averaged over all classes and is denoted as AR. The ARmax=1 and ARmax=10 are considered, which are calculated for 1 detection and a maximum of 10 detections in one image, respectively. Both ARs are averaged over the described 10 IoU thresholds.

CQ23: What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?
Answer: To prevent overfitting and to generalize better, different data augmentation techniques were utilized during the training process. The python library imgaug was used for the data augmentation of images and videos. For Mask R-CNN and FGFA, the same augmentation technique with identical parameters was applied to each frame of the video. The augmentation techniques used were horizontal flip, vertical flip, and rotation.

CQ24: What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?
Answer: The context does not provide information on strategies employed to handle randomness in the deep learning pipeline, such as random seed values.

CQ25: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is to combine (1) the detection of individual animals yielding bounding boxes as well as precise shapes of appearances with (2) the recognition of the actions of the observed animals. The resulting end-to-end pipeline is applicable in camera traps for tracing of animal populations and analysing their abundance and behaviour. %Query

CQ26: What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?
Answer: The described SlowFast architecture uses balance weights as (1. 0,4. 5,4. 5) for the classes eating, moving and watching to account for the unbalanced frequencies of action classes in the dataset.

CQ27: What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?
Answer: The context does not provide information on the process followed to deploy the trained deep learning model, including details on model serialization or platform selection.

CQ28: Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?
Answer: The context does not provide information on the platform used to deploy the deep learning model.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ... 