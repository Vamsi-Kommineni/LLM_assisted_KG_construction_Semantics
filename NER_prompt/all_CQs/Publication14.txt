 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ1: What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)?
Answer: The context does not provide specific information on the methods utilized for collecting raw data in the deep learning pipeline. However, it does mention that the author and their collaborators have been studying hummingbird ecology using time-lapse cameras in the Ecuadorian Andes since 2013. These cameras turn on at dawn, off at dusk, and record a photo every second for up to 5 days. Cameras filming individual flowers capture hummingbirds in less than 1% of images. This suggests that time- lapse cameras are used for data collection in this specific case.

CQ2: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?
Answer: The deep learning pipeline uses image data for training and classification. %Context layer reduces the vector of image features to the desired dimension- ality of length two (foreground and background). The softmax layer normalises this vector into probabilities that sum to one across all classes. DeepMeerkat is designed to be conservative, with a high F I G U R E 2 The front screen of the DeepMeerkat GUI. A user can select a file or directory of videos to screen using a pre- trained model. The path to the model is set under “Advanced settings” threshold for retaining frames (acceptance value = 0. 1). This means that the model must be more than 90% confident that a frame does not contain a foreground object to assign a background label. This prioritises minimising false negatives at the potential expense of in- majority of hummingbird visitation events (Weinstein, 2015). For the purposes of this article, I assumed that all events are captured by For training the fine- tuned neural network, I collected images for each class and trained with a batch size of 100 for 20,000 steps. To reduce training time, the feature vectors for the frozen layers were extracted in parallel using Google Cloud DataFlow. These features were then the basis for retraining the new fine- tuned layers. To fit the specifications of the pre- trained frozen layers, the bounding boxes from motion detection were resized into three channel arrays with height and width of 299 pixels. Following Zhang, He, Cao, and Cao (2016), aspect ratios of bounding boxes were not maintained when passing boxes to the neural network. Model performance was measured using true positive rate, true negative rate, and precision. A DeepMeerkat GUI (Figure 2, Figures S1 and S2) is available for download for Mac and Windows with the pre- trained humming- 3 | R E S U L T S Deng, J. , Dong, W. , Socher, R. , Li, L-J. , Li, K. , & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, (June), (pp. 248–255). https://doi. org/10. 1109/cvprw. 2009. 5206848 Elias, A. R. , Golubovic, N. , Krintz, C. , & Wolski, R. (2017). Where’s the bear? – Automating wildlife image processing using IoT and edge WEINSTEIN 2041210x, 2018, 6, Downloaded from https://besjournals. onlinelibrary. wiley. com/doi/10. 1111/2041-210X. 13011 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary. com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License | 1441 Methods in Ecology and Evolu(cid:13)on Wilf, P. , Zhang, S. , Chikkerur, S. , Little, S. A. , Wing, S. L. , & Serre, T. (2016). Computer vision cracks the leaf code. Proceedings of the National Academy of Sciences of the United States of America, 113, 201524473. Zhang, Z. , He, Z. , Cao, G. , & Cao, W. Animal detection from highly cluttered natural scenes using spatiotemporal object region propos- als and patch verification. IEEE Transactions on Multimedia, 18, 2079– 2092. 1109/TMM. 2016. 2594138 How to cite this article: Weinstein BG. Scene- specific convolutional neural networks for video- based biodiversity detection. Methods Ecol Evol. 2018;9:1435–1441. 13011 S U P P O R T I N G I N FO R M AT I O N Additional supporting information may be found online in the Supporting Information section at the end of the article. Helpful

CQ3: What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?
Answer: The data annotation techniques used in the deep learning pipeline include bounding box annotation and instance segmentation. Bounding box annotation involves drawing a box around an object in an image to identify its location, while instance segmentation involves labeling each pixel in an image to identify the object it belongs to. These techniques are used to create labeled datasets that can be used to train deep learning models. %Context Context: Deng, J. , Dong, W. , Socher, R. , Li, L-J. , Li, K. , & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, (June), (pp. 248–255). https://doi. org/10. 1109/cvprw. 2009. 5206848 Elias, A. R. , Golubovic, N. , Krintz, C. , & Wolski, R. (2017). Where’s the bear? – Automating wildlife image processing using IoT and edge WEINSTEIN 2041210x, 2018, 6, Downloaded from https://besjournals. onlinelibrary. wiley. com/doi/10. 1111/2041-210X. 13011 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary. com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License | 1441 Methods in Ecology and Evolu(cid:13)on Wilf, P. , Zhang, S. , Chikkerur, S. , Little, S. A. , Wing, S. L. , & Serre, T. (2016). Computer vision cracks the leaf code. Proceedings of the National Academy of Sciences of the United States of America, 113, 201524473. Zhang, Z. , He, Z. , Cao, G. , & Cao, W. Animal detection from highly cluttered natural scenes using spatiotemporal object region propos- als and patch verification. IEEE Transactions on Multimedia, 18, 2079– 2092. 1109/TMM. 2016. 2594138 How to cite this article: Weinstein BG. Scene- specific convolutional neural networks for video- based biodiversity detection. Methods Ecol Evol. 2018;9:1435–1441. 13011 S U P P O R T I N G I N FO R M AT I O N Additional supporting information may be found online in the Supporting Information section at the end of the article. download for Mac and Windows with the pre- trained humming- 3 | R E S U LT S bird model. In addition, I provide reproducible scripts for local and Google cloud environments to allow users to train new models, Feature extraction of the fixed inception layers completed in 1 hr which can then be used in the local software. and 26 min on 15 CPUs. Training of the new layers completed in 27 min on a single CPU. Model evaluation on the 70 test videos com- 2. 2 | Test dataset pleted in 4 hr and 38 min on 30 CPUs with an average frame rate of 17 frames/s. On average, a video contained 545. 84 candidate mo- My collaborators and I have been studying hummingbird ecol- ogy using time- lapse cameras in the Ecuadorian Andes since 2013 (Weinstein & Graham, 2017). Cameras turn on at dawn, off at dusk, and record a photo every second for up to 5 days. Cameras filming individual flowers capture hummingbirds in less than 1% of images. Hummingbird visits are rapid and rare, lasting 3 to 5 s, with only a handful of visits a day. To train the network, I collected 14,432 image crops containing hummingbirds and 14,432 crops containing back- ground vegetation and sky. To validate the accuracy of the model, I selected 70 half- day videos that represented a range of challeng- ing backgrounds. These videos were not used in the model training. Within these videos there 532 frames (1. 4%) containing humming- birds and 37,677 frames (98. 6%) containing background. Previous analysis showed motion detection to be effective in finding the motion objects based on training data, and can be improved over time with newly labelled data. DeepMeerkat is an extension of MotionMeerkat (Weinstein, 2015), with the addition of a neural network to classify movement objects as either foreground or background. Rather than train a novel ing images based on temporal filtering (Swinnen, Reijniers, Breno, Fingal, & Van Gool, 2016), I used a convolutional neural network (CNN) to classify objects based on their visual features. CNNs are a type of deep learning algorithm that have been successful in image classification tasks (Krizhevsky, Sutskever, & Hinton, 2012). CNNs consist of multiple layers of filters that are applied to the input image to extract features at different levels of abstraction. The final layer of the CNN is a fully connected layer that maps the extracted features to a set of class labels. To train the CNN, I used a dataset of 14,432 images containing hummingbirds and 14,432 images containing background vegetation and sky. The images were collected using time-lapse cameras in the Ecuadorian Andes. The images were cropped to a size of 224 × 224 pixels and labelled as either containing a hummingbird or not. The dataset was split into training and validation sets, with 80% of the images used for training and 20% used for validation. The CNN was trained using the TensorFlow library (Abadi et al. , 2016) on a single NVIDIA Tesla K80 GPU. The network was trained for 50 epochs with a batch size of 32 and a learning rate of 0. 001. The network achieved an accuracy of 98. 6% on the validation set. To evaluate the performance of the CNN on new videos, I selected 70 half-day videos that represented a range of challenging backgrounds. Within these videos there were 532 frames (1. 4%) containing hummingbirds and 37,677 frames (98. The CNN was able to correctly classify 95. 7% of the hummingbird frames and 76. 1% of the background frames using a conservative acceptance threshold of 0. 1. The trade-off between recall and precision varied by acceptance value, with a faster decrease in background recall at low acceptance values (Figure 3). The majority of motion detection objects were well centred and had high foreground classification probabilities (Figure 4). Using the more conservative 0. 1 acceptance value, the remaining misclassified frames (n = 24) had poor bounding box segmentation (Figure 5). Helpful

CQ4: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?
Answer: The context does not provide information on the specific data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide a helpful answer to this query.

CQ5: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)?
Answer: The deep learning pipeline used the 2012 ImageNet data for training the Inception V3 model. The model was then fine-tuned using a smaller set of training images specific to the ecological project. The specific datasets used for fine-tuning are not mentioned in the provided context.

CQ6: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide specific information on the preprocessing steps involved before training the deep learning model. However, common preprocessing steps for deep learning models include normalization, scaling, and cleaning. Normalization scales the data to a specific range, typically between 0 and 1, to ensure that all features have the same scale. Scaling adjusts the range of the data, often to a standard range such as -1 to 1 or 0 to 1. Cleaning involves removing or correcting any errors or inconsistencies in the data. These steps are important to ensure that the model can accurately learn from the data and make accurate predictions.

CQ7: What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?
Answer: The context does not provide explicit information on the criteria used to split the data for deep learning model training. However, it mentions that 70 half-day videos were selected for validation, which were not used in the model training. This suggests that the data was split into training and validation sets. The context does not mention a test set, but it is common practice in machine learning to split the data into training, validation, and test sets. The criteria for splitting the data can vary, but common methods include random sampling or stratified sampling to ensure that the distribution of classes is similar in all sets.

CQ8: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?
Answer: The code repository of the deep learning pipeline is available on GitHub.

CQ9: Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?
Answer: The data repository of the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, I cannot provide the specific data repository where the deep learning pipeline is available.

CQ10: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: The code repository link of the deep learning pipeline is not provided in the given context.

CQ11: What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?
Answer: I'm sorry, but the provided context does not include information about the data repository link of the deep learning pipeline.

CQ12: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a convolutional neural network (CNN) model, specifically Google's Inception V3 model, which was originally trained on the 2012 ImageNet data. The model is fine-tuned for the specific task by freezing the lower layers of the architecture and retraining the top layers to better fit the new dataset.

CQ13: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The context does not provide information on the hyperparameters used in the deep learning model, such as the learning rate or optimizer.

CQ14: How are the hyperparameters of the model optimized (e.g., grid search, random search)?
Answer: The context does not provide information on how the hyperparameters of the model are optimized. Therefore, I don't know the answer to this question.

CQ15: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The context does not provide information on the specific optimization techniques applied in the deep learning pipeline. Therefore, I cannot provide an answer to this query.

CQ16: What criteria are used to determine when training is complete (e.g., validation loss plateau)?
Answer: The criteria used to determine when training is complete are not explicitly mentioned in the provided context. However, it is mentioned that the model was trained with a batch size of 100 for 20,000 steps. This suggests that the number of training steps might be used as a criterion to determine when training is complete.

CQ17: What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The paper "Dropout: A simple way to prevent neural networks from overfitting" by Srivastava et al. (2014) discusses the use of dropout as a regularization method to prevent overfitting in neural networks. The paper "Deep background subtraction with scene-specific convolutional neural networks" by Braham and Van Droogenbroeck (2016) also mentions the use of L2 regularization as a regularization method.

CQ18: What is the strategy implemented to monitor the model performance during training?
Answer: The strategy implemented to monitor the model performance during training is not explicitly mentioned in the provided context. However, it is mentioned that the model evaluation on the 70 test videos completed in 4 hr and 38 min on 30 CPUs with an average frame rate of 17 frames/s. This suggests that the model performance was monitored during training by evaluating it on a test dataset.

CQ19: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The deep learning model is built using TensorFlow.

CQ20: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context does not provide specific information about the hardware resources used for training the deep learning model. However, it does mention that feature extraction of the fixed inception layers completed in 1 hr and 26 min on 15 CPUs, and training of the new layers completed in 27 min on a single CPU. This suggests that the model was trained using CPUs.

CQ21: What are the postprocessing steps involved after the model training (e.g., Saliency maps, Metrics calculation, Confusion matrix)?
Answer: The context does not provide information on the postprocessing steps involved after the model training. Therefore, I don't know the answer to this query.

CQ22: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance of the deep learning model is evaluated using true positive rate, true negative rate, and precision.

CQ23: What measures were taken to ensure the generalizability of the deep learning model (e.g., Diverse dataset, cross-validation, Stratified splitting)?
Answer: The text does not provide explicit information on the measures taken to ensure the generalizability of the deep learning model. However, it does mention that the model was trained using a diverse dataset of 14,432 image crops containing hummingbirds and 14,432 crops containing background vegetation and sky. Additionally, the model was validated using 70 half-day videos that represented a range of challenging backgrounds and were not used in the model training. These steps suggest that efforts were made to ensure the model's generalizability.

CQ24: What strategies are employed to handle randomness in the deep learning pipeline (e.g., random seed value)?
Answer: The context does not provide information on strategies employed to handle randomness in the deep learning pipeline, such as random seed values. Therefore, I don't know the answer to this query based on the provided context.

CQ25: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is for classification. It is used to classify frames as either containing a foreground object (hummingbird) or background objects (vegetation and sky).

CQ26: What techniques are used to address data bias during preprocessing of the deep learning pipeline (e.g., Stratified splitting, oversampling, undersampling, Diverse data collection)?
Answer: The context does not provide information on specific techniques used to address data bias during preprocessing of the deep learning pipeline. However, common techniques include stratified splitting, oversampling, undersampling, and diverse data collection. Stratified splitting ensures that the training and testing sets have a similar distribution of classes. Oversampling and undersampling are techniques used to balance the class distribution in the dataset. Diverse data collection involves collecting data from various sources to ensure that the model generalizes well to new data.

CQ27: What process was followed to deploy the trained deep learning model (e.g., Model serialization, Platform selection)?
Answer: The context does not provide information on the process followed to deploy the trained deep learning model, including details on model serialization or platform selection.

CQ28: Which platform was used to deploy the deep learning model (e.g., AWS, Azure, Google Cloud platform)?
Answer: The context does not provide information on the specific platform used to deploy the deep learning model.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ... 