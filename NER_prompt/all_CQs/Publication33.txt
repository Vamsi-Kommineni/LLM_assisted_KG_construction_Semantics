 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ1: What methods are utilized for collecting raw data in the deep learning pipeline (e.g., surveys, sensors, public datasets)?
Answer: The methods utilized for collecting raw data in the deep learning pipeline include surveys, sensors, and public datasets. For instance, remote sensors such as satellites, drones, camera traps, or underwater cameras are used to record pictures or videos of aquatic and terrestrial organisms. Additionally, public datasets like Fish4Knowledge database and Seaclef challenge are used for ecological analysis.

CQ2: What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?
Answer: The data formats used in the deep learning pipeline are images. The images are extracted from videos and are coded as 3 matrices with numeric values describing the color component (R, G, B) of the pixel.

CQ3: What are the data annotation techniques used in the deep learning pipeline (e.g., bounding box annotation, instance segmentation)?
Answer: The context does not provide specific information on the data annotation techniques used in the deep learning pipeline. However, it mentions that the training phase is supervised, which means that the classes to identify are pre-defined by human experts. This suggests that some form of annotation was used to label the training data, but the specific techniques are not mentioned.

CQ4: What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?
Answer: The data augmentation techniques applied in the deep learning pipeline are increasing and decreasing contrast. Each "natural" image yielded 4 more images: 2 with increased contrast (120% and 140%) and 2 with decreased contrast (80% and 60%). Additionally, all images were horizontally flipped to obtain the final training dataset.

CQ5: What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)?
Answer: The datasets used in the deep learning pipeline are T0, T1, and T2. T0 is the training dataset composed of 691,690 images, T1 is the dataset used to tune the thresholds containing 6,320 images, and T2 is the test dataset containing 13,232 images. These datasets are not standard datasets like MNIST, CIFAR, or ImageNet, but are specifically created for this study case to identify 20 fish species from underwater images on coral reefs.

CQ6: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: Before training the deep learning model, the images used to build the datasets were resized to 64 × 64 pixels. Additionally, data augmentation was used to improve the model. Each "natural" image yielded 4 more images: 2 with increased contrast (120% and 140%) and 2 with decreased contrast (80% and 60%). All images were then horizontally flipped to obtain the final training dataset.

CQ7: What are the criteria used to split the data for deep learning model training (e.g., train, test, validation)?
Answer: The context does not provide explicit information on the criteria used to split the data for deep learning model training. However, it is mentioned that the training procedure lasted 600,000 iterations, and each iteration processed a batch of 16 images. The 691,690 images of the training dataset were analyzed 14 times each by the network on average. The training was stopped to prevent overfitting. It is also mentioned that the dataset T1 was used for the threshold-setting phase and the dataset T2 for the testing phase, and that the roles of T1 and T2 were switched to assess the robustness of the method.

CQ8: Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?
Answer: The code repository of the deep learning pipeline is not mentioned in the provided context. Therefore, I don't know where it is available.

CQ9: Where is the data repository of the deep learning pipeline available (e.g., Zenodo, Figshare, Dryad, GBIF)?
Answer: The data repository of the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, I cannot provide the specific data repository where the deep learning pipeline is available.

CQ10: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, but the provided context does not mention the code repository link of the deep learning pipeline.

CQ11: What is the data repository link of the deep learning pipeline (e.g., Link to Zenodo, Figshare, Dryad, GBIF)?
Answer: I'm sorry, the provided context does not mention a data repository link for the deep learning pipeline.

CQ12: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a 100-layer CNN based on the TensorFlow implementation of ResNet.

CQ13: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The provided context does not mention the specific hyperparameters used in the deep learning model, such as the learning rate or optimizer.

CQ14: How are the hyperparameters of the model optimized (e.g., grid search, random search)?
Answer: The hyperparameters of the model are optimized through a process called back-propagation. Back- propagation consists of automatically changing parameters of the CNN through the comparison between its output and the correct class of the training element to eventually improve the final classification rate. The training procedure lasted 600,000 iterations; each iteration processed a batch of 16 images, which means that the 691,690 images of the training dataset were analyzed 14 times each by the network on average. The training was then stopped to prevent overfitting, as an overfit model is too restrictive and only able to classify images that were used during the training.

CQ15: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The optimization of the parameters of the CNN is achieved during the training through a process called back-propagation. Back-propagation consists of automatically changing parameters of the CNN through the comparison between its output and the correct class of the training element to eventually improve the final classification rate. Here we used a 100-layer CNN based on the TensorFlow implementation of ResNet. The ResNet architecture achieved the best results on ImageNet Large Scale Visual Recognition Competition (ILSVRC) in 2015, considered as the most challenging image classification competition. It is still one of the best classification algorithms, while being very easy to use and implement. All fish images extracted from the videos to build our datasets were resized to 64 × 64 pixels before being processed by the CNN. Our training procedure lasted 600,000 iterations; each iteration processed a batch of 16 images, which means that the 691,690 images of the training dataset were analyzed 14 times each by the network on average. We then stopped the training to prevent from overfitting, as an overfit model is too restrictive and only able to classify images that were used during the training. The context does not provide specific information on the optimization techniques applied in the deep learning pipeline, such as SGD or Adam. However, it mentions that the optimization of the parameters of the CNN is achieved during the training through a process called back-propagation. The context also mentions the use of a 100-layer CNN based on the TensorFlow implementation of ResNet, which achieved the best results on ImageNet Large Scale Visual Recognition Competition (ILSVRC) in 2015. The training procedure lasted 600,000 iterations, with each iteration processing a batch of 16 images. The training was stopped to prevent overfitting.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ... 