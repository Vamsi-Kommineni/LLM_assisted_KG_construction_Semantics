 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The deep learning pipeline uses audio data. The audio data is converted to a Mel spectrogram representation with 32 frequency bins, represented as rows in the spectrogram, using a temporal resolution of 21 columns per second of raw audio. Each column in the spectrogram is computed by running the fast Fourier transform on a section of the audio time signal. Each spectrogram column is computed from 0. 0928 s of audio (which corresponds to a window size of 2,048 samples), and has a Hann window applied. The columns are extracted from the audio signal at a frequency of 21. 53 Hz (or equivalently with a hop length of 1,024 audio samples, on our 22,050 Hz audio). Before use in the classifier, the values of the spectrogram S are converted to a log-scale representation, using the formula ln(A + B * S). For CityBioNet the parameters A = 0. 001 and B = 10. 0 are used, while for CityAnthroNet the parameters A = 0. 025 and B = 2. 0 are used. These parameters are chosen manually to emphasise biotic and anthropogenic sounds by visually inspecting the transformed spectrograms.

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Fliping, Roating, Scaling)?
Answer: The context does not provide information on the specific data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide a helpful answer to this query.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The deep learning pipeline uses the CitySounds2017 dataset, an expert-annotated dataset of urban sounds collected across Greater London, UK. This dataset was used to train the CNNs, CityBioNet and CityAnthroNet, for measuring audible (0-12 kHz) biotic and anthropogenic acoustic activity in audio recordings from urban environments.

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The preprocessing steps involved before training a deep learning model include normalization, scaling, and cleaning. Normalization is a technique often applied as part of data preparation for machine learning. It is a process that changes the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values or losing information. Scaling is a feature scaling technique in which values are rescaled so that they end up ranging between 0 and 1. Cleaning is the process of removing or correcting erroneous data from a dataset. %Context work consisted of a 32 × 21 × 4 tensor. Foundation, 2016) using theano v. 0. 9. 0 (The Theano Development 5. Apply CNN classifier: As described above, classification was per- Team, et al. 2016) and lasagne v. 2 (Dieleman et al. , 2015) for ML and librosa v. 4. 2 (McFee et al. , 2015) for audio processing. formed with a CNN, whose parameters were learnt from training data. The CNN comprised a series of layers, each of which modi- fied its input data with parameterised mathematical operations 2. 1 | Acoustic dataset which were optimised to improve classification performance dur- We selected 63 green infrastructure (GI) sites in and around Greater ing training (see Supplementary Methods for details). The final London, UK to collect audio data to train and test the CityNet layer produced the prediction of presence or absence of biotic or Audible sounds Ground truth One second of audio Anthropogenic: Yes Biotic: Yes Anthropogenic: Yes Biotic: No Anthropogenic: No Biotic: Yes Anthropogenic: No Biotic: No F I G U R E 2 The four acoustic states predicted by the CityNet algorithms. Each 1 s chunk of audio may contain anthropogenic and biotic sound (top row), just anthropogenic sound (second row), just biotic sound (third row), or neither biotic nor anthropogenic sound (final row). CityBioNet and CityAnthroNet were independently used to detect presence or absence of biotic and anthropogenic sounds, allowing each chunk of audio to be categorised into one of four states FAIRBRASS et Al. 2041210x, 2019, 2, Downloaded from https://besjournals. onlinelibrary. wiley. com/doi/10. 1111/2041-210X. 13114 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary. com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 190 | Methods in Ecology and Evolu(cid:13)on algorithms. These sites represent a range of GI in and around Greater the four potential acoustic states that CityNet algorithms can London in terms of GI type, size and urban intensity. Each site was sam- (Figure 2) were represented. CitySounds2017 test was sampled from different recording sites to CitySounds2017train to demonstrate that the CityNet algorithms generalise to sounds predict pled for seven consecutive days across the months of May to October between 2013 and 2015 (Figure 3, Supporting Information Table S1). Repeat for each window in spectrogram 2 | M ATE R I A L S A N D M E TH O DS We developed two CNN models, CityBioNet and CityAnthroNet 5 CNN within the CityNet system to generate measures of biotic and an- thropogenic acoustic activity respectively. The CityNet pipeline (Figure 1) consisted of seven main steps as follows: 6 Biotic response 1. Record audio: Audible frequency (0–12 kHz). wav recordings were made using a passive acoustic recorder at a sample rate of 24 kHz. Time 2. Audio conversion to Mel spectrogram: Each audio file was automati- cally converted to a Mel spectrogram representation with 32 fre- 7 quency bins, represented as rows in the spectrogram, using a Average biotic response temporal resolution of 21 columns per second of raw audio. Each column in the spectrogram was computed by running the fast Fourier transform on a section of the audio time signal. Each spec- M onday S aturday Friday S unday T hursday W ednesday T uesday trogram column was computed from 0. 0928 s of audio (which cor- responds to a window size of 2,048 samples), and has a Hann window applied. The columns were extracted from the audio sig- Day of week nal at a frequency of 21. 53 Hz (or equivalently with a hop length F I G U R E 1 The CityNet analysis pipeline for measuring biotic and anthropogenic acoustic activity. Raw audio (1), recorded in the field, is converted to a spectrogram representation (2). A sliding window is run across the time dimension, and a window of the spectrogram extracted at each step (3). This spectrogram window is preprocessed with four different normalisation strategies, and the results concatenated. This stack of spectrograms is passed through a CNN (5), which was trained on CitySounds2017train. The CNN gives, at each 1- s time step, a prediction of the presence/absence of biotic or anthropogenic acoustic activity (6). Finally, these per- time- step measures can be aggregated to give summaries over time or space (7) of 1,024 audio samples, on our 22,050 Hz audio). Before use in the classifier, the values of the spectrogram S was converted to a log-scale representation, using the formula ln(A + B * S). For CityBioNet the parameters A = 0. 001 and B = 10. 0 were used, while for CityAnthroNet the parameters A = 0. 025 and B = 2. 0 were used. These parameters were chosen manually to emphasise biotic and anthropogenic sounds by visually inspecting the trans- | 191 Methods in Ecology and Evolu(cid:13)on (a) (b) (c) F I G U R E 3 Location of study sites and average daily acoustic patterns at two sites along an urbanisation gradient. Points in (a) represent locations used for the training dataset, CitySounds2017train (black) and testing dataset, CitySounds2017test (red). Here CityNet was run across the entire 7 days of recording at two sites of high (b) and low (c) urban intensity to predict the presence/absence of biotic and anthropogenic sound at each second of the week using a threshold of 0. 5. The predicted number of seconds containing biotic and anthropogenic sound for each half- hour period was averaged over the week to produce average daily patterns of acoustic activity. Greater London boundary indicated with bold line. Boundary data from the UK Census (http://www. ons. gov. uk/, accessed 04/11/2014) FAIRBRASS et Al. com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License 192 | Methods in Ecology and Evolu(cid:13)on absence of biotic sound at threshold 0. 5 was correctly predicted 2. 4 | Competing algorithms in the full and subset datasets by each algorithm, and the Cramer’s V statistic was used to assess the effect size of differences as this We also compared the precision and recall of the CityNet algorithms to acoustic measures produced by four Acoustic Indices: Acoustic Complexity Index (ACI) (Pieretti et al. , 2011), Acoustic Diversity Index (ADI) (Villanueva- Rivera et al. , 2011), Bioacoustic Index (BI) (Boelman et al. , 2007), and Normalised Difference Soundscape 2. 6 | Ecological application Index (NDSI) (Kasten et al. , 2012). The NDSI generates a measure of anthropogenic disturbance according to the formula: We used CityNet to generate daily average patterns of biotic and anthropogenic acoustic activity for two study sites across an urbani- sation gradient (sites E29RR and IG62XL with high and low urbani- sation, respectively, Supporting Information Table S1). To control were computed for the average precision under all the threshold However, advances in low- shot learning (e. g. , Wang, Girshick, Hebert, & Hariharan, 2018) may allow our annotations to be used to create such a fine- grained classifier in the future. 2. 3 | Acoustic testing dataset and evaluation 2. 5 | Performance evaluation To evaluate the performance of the CityNet algorithms, we cre- ated a testing dataset (CitySounds2017test) by selecting 40 1- min recordings from CitySounds2017 from the remaining 30% of sites (19 sites, average 2 ± 1 recordings per site). The testing dataset was randomly selected from the remaining sites so that To evaluate the performance of the CityNet algorithms, we cre- ated a testing dataset (CitySounds2017test) by selecting 40 1- min recordings from CitySounds2017 from the remaining 30% of sites (19 sites, average 2 ± 1 recordings per site). The testing dataset was randomly selected from the remaining sites so that the testing dataset was representative of the full dataset. The the testing dataset was representative of the full dataset. The performance of the CityNet algorithms was evaluated by comparing performance of the CityNet algorithms was evaluated by comparing the predicted presence/absence of biotic and anthropogenic sound the predicted presence/absence of biotic and anthropogenic sound to the ground truth labels. The ground truth labels were generated to the ground truth labels. The ground truth labels were generated by manually annotating the testing dataset. The annotations were by manually annotating the testing dataset. The annotations were made by two independent annotators, and any disagreements were resolved by a third annotator. The annotations were made by two resolved by a third annotator. The annotations were made by two independent annotators, and any disagreements were resolved by a independent annotators, and any disagreements were resolved by a third annotator. The annotations were made at a 1- s time resolution, third annotator. The annotations were made at a 1- s time resolution, and the annotators were instructed to label the presence/absence of and the annotators were instructed to label the presence/absence of biotic and anthropogenic sound. The annotations were made at a biotic and anthropogenic sound. The annotations were made at a 1- s time resolution, and the annotators were instructed to label the 1- s time resolution, and the annotators were instructed to label the presence/absence of biotic and anthropogenic sound. The annotations presence/absence of biotic and anthropogenic sound. The annotations were made at a 1- s time resolution, and the annotators were instructed were made at a 1- s time resolution, and the annotators were instructed to label the presence/absence of biotic and anthropogenic sound. The to label the presence/absence of biotic and anthropogenic sound. The annotations were made at a 1- s time resolution, and the annotators annotations were made at a 1- s time resolution, and the annotators were instructed to label the presence/absence of biotic and anthropogenic were instructed to label the presence/absence of biotic and anthropogenic sound. The annotations were made at a 1- s time resolution, and the sound. The annotations were made at a 1- s time resolution, and the annotations were made at a 1- s time resolution, and the annotators annotations were made at a 1- s time resolution, and the annotators were instructed to label the presence/absence of biotic and anthropogenic were instructed to label the presence/absence of biotic and anthropogenic sound. The annotations were made at a 1- s time resolution, and the annotations were made at a 1- s time resolution, and the an

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: The code repository link of the deep learning pipeline is not provided in the given context.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a Convolutional Neural Network (CNN) model.

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The helpful answer is that the hyperparameters used in the deep learning model are not explicitly mentioned in the provided context. Therefore, it is not possible to provide a specific answer to this query based on the given information. Final

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The helpful answer is that the optimization technique applied in the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, it is not possible to determine which optimization technique was used. %Query

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The context does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline. Therefore, I don't know the answer to this query.

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The deep learning model is built using Theano v. 0. 9. 0 and lasagne v. 2 for machine learning, and librosa v. 4. 2 for audio processing.

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context does not provide information on the specific hardware resources used for training the deep learning model. Therefore, I cannot provide an answer to this query.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance of the deep learning model is evaluated using precision and recall metrics. The fraction of true positives, false positives, true negatives, and false negatives were also computed, using the same threshold. These analyses were conducted in python v. 2. 7. 12 using scikit-learn v. 0. 18. 1 and matplotlib v. 1. 5. The experiments were run on a machine running Ubuntu 16. 04 with a 3. 60 GHz Xeon processor, 64 GB of RAM, and a 2 GB Nvidia GPU.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is to generate measures of biotic and anthropogenic acoustic activity. The model is a convolutional neural network (CNN) and is part of the CityNet system. The CNN is trained on a large dataset of annotated audio recordings collected across Greater London, UK. The CNN is used to predict the presence or absence of biotic or anthropogenic acoustic activity in each 1-second chunk of audio. The predictions can then be aggregated to give summaries over time or space.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...