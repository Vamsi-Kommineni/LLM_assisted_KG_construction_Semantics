 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The deep learning pipeline in this context uses image data format. %Explanation Explanation: The context mentions that the deep learning pipeline uses image data format. It does not mention any other data formats such as audio, video, or CSV. Therefore, the answer is that the deep learning pipeline in this context uses image data format. %Query

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Fliping, Roating, Scaling)?
Answer: The data augmentation techniques applied in the deep learning pipeline are: Random scale, Random crop, Flip horizontally, and Random brightness.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The datasets used in the deep learning pipeline are not explicitly mentioned in the provided context. However, it is mentioned that the GoogLeNet classifier was initialized with the pre-trained weights of ImageNet, which suggests that ImageNet was used as a dataset in the deep learning pipeline.

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide information on the specific preprocessing steps involved before training a deep learning model. However, in general, preprocessing steps can include normalization, scaling, cleaning, and data augmentation techniques such as random scale, random crop, flip horizontally, and random brightness.

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: The code repository link of the deep learning pipeline is not provided in the given context.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a deep learning model called GoogLeNet, which is a type of Convolutional Neural Network (CNN).

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The helpful answer is that the hyperparameters used in the deep learning model are not explicitly mentioned in the provided context. However, it is mentioned that the Stochastic Gradient Descent (SGD) is commonly used to update the weights, and the network weights can be randomly initialized if the network is trained from scratch. Additionally, transfer learning (e. g. , fine-tuning in CNNs) is used to initialize the weights of the network by the pre-trained weights on a different dataset.

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The Stochastic Gradient Descent (SGD) is commonly used to update the weights in the deep learning pipeline. The network weights, wt, can be randomly initialized if the network is trained from scratch. However, this is suitable only when a large labeled training-set is available. Several works have shown that data-augmentation and transfer learning help overcome this limitation. Transfer learning (e. g. , fine-tuning in CNNs) consists of re-utilizing the knowledge learned from one problem to another related one. In deep CNNs, transfer learning can be applied via fine-tuning, which involves initializing the weights of the network by the pre-trained weights on a different dataset.

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The regularization methods used to prevent overfitting in the deep learning pipeline are dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. L2 regularization, also known as weight decay, is a method that adds a penalty term to the loss function, which discourages large weights and helps to prevent overfitting.

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The deep learning model is built using TensorFlow. %Explanation Explanation: The context mentions that TensorFlow is used for large-scale machine learning on heterogeneous distributed systems. This implies that TensorFlow is the framework used to build the deep learning model.

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context does not provide information on the specific hardware resources used for training the deep learning model.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The performance of the deep learning model is evaluated using three metrics: precision (also called positive predictive value, i. e. , how many detected Ziziphus are true), recall (also known as sensitivity, i. , how many actual Ziziphus were detected), and F1 measure, which evaluates the balance between precision and recall.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is for supervised classification. It is used to automatically discover increasingly higher level features from data. The lower convolutional layers capture low-level image features, e. g. edges, color, while higher convolutional layers capture more complex features, i. e. , composite of several features. The deep learning model used in this work is GoogLeNet, which was the winner of ILSVRC (ImageNet Large Scale Visual Recognition Competition (ILSVRC)) 2014. It provides higher accuracy with less computational cost compared to previous network architectures.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...