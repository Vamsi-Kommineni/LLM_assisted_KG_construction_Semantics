 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The deep learning pipeline uses image data formats, specifically spectrograms, for the classification challenges.

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Fliping, Roating, Scaling)?
Answer: The context does not provide information on the specific data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide a helpful answer to this query.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The datasets used in the deep learning pipeline are not explicitly mentioned in the provided context. However, it is mentioned that TensorFlow was trained on all images (i. e. , 400 spectrograms) from Challenge 1 as an image reference library. Additionally, it is mentioned that Inception v3 was trained on > 1 M images from the ImageNet database. Therefore, the datasets used in the deep learning pipeline are spectrograms and ImageNet.

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The context does not provide specific information on the preprocessing steps involved before training a deep learning model. However, common preprocessing steps for deep learning models include normalization, scaling, and cleaning of the data. Normalization is the process of adjusting the values of numeric columns in the dataset to a common scale, without distorting the ranges of values or losing information. Scaling is the process of rescaling the values of numeric columns in the dataset to a standard range, such as 0 to 1 or -1 to 1. Cleaning is the process of removing or correcting errors, inconsistencies, and missing values in the dataset. These preprocessing steps can help improve the performance and accuracy of the deep learning model.

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: The code repository link of the deep learning pipeline is not provided in the given context.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The pipeline uses a Convolutional Neural Network (CNN) model, specifically Inception v3, implemented in TensorFlow.

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The context does not provide information on the hyperparameters used in the deep learning model, such as learning rate or optimizer.

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The context does not provide information on the specific optimization techniques applied in the deep learning pipeline.

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The provided context does not mention any specific regularization methods used to prevent overfitting in the deep learning pipeline, such as dropout or L2 regularization. Therefore, I cannot provide an answer to this query based on the given context.

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The deep learning model is built using TensorFlow.

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The context does not provide information on the hardware resources used for training the deep learning model.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The metrics used to evaluate the performance of the deep learning model are the number of images per class assigned to a species present in the reference library and the average certainty rate associated with those identifications.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is to classify species of Philippine forest frogs based on their frequency distributions of individual call notes.

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...