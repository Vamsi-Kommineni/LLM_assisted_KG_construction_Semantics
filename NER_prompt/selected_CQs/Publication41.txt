 %INSTRUCTIONS: Your task is to do Named Entity Recognition. You extract named entities from the provided competency question answers. 
        Use provided concepts to understand which named entities to extract from competency answers. 
        Concepts: Method, RawData, DataFormat, DataAnnotationTechnique, DataAugmentationTechnique, Dataset, PreprocessingStep, DataSplitCriteria, CodeRepository, DataRepository, CodeRepositoryLink, DataRepositoryLink, DeepLearningModel, Hyperparameter, HyperparameterOptimization, OptimizationTechnique, TrainingCompletionCriteria, RegularizationMethod, ModelPerformanceMonitoringStrategy, Framework, HardwareResource, PostprocessingStep, PerformanceMetric, GeneralizabilityMeasure, RandomnessStrategy, ModelPurpose, DataBiasTechnique, ModelDeploymentProcess, DeploymentPlatform.         
        Example: DeepLearningModel(CNN, RNN, Transformer), Framework(TensorFlow, PyTorch), PerformanceMetric(Accuracy, mean IoU)
        Below are the competency questions and answers: 
CQ: What data formats are used in the deep learning pipeline (e.g, image, audio, video, csv)?
Answer: The dataset for this work has been derived from Planets full-frame analytic scene products using its 4-band satellites in sun-synchronous orbit (SSO) and International Space Station (ISS) orbit. The data format used in the deep learning pipeline is Chip (Image) Data Format.

CQ: What are the data augmentation techniques applied in the deep learning pipeline (e.g, Fliping, Roating, Scaling)?
Answer: The paper does not provide specific details on the data augmentation techniques applied in the deep learning pipeline. However, it does mention that data augmentation could be performed to make the system more robust, considering that satellite images may vary in terms of lighting effect, rotation, shifting, etc. Therefore, it can be inferred that techniques such as flipping, rotation, and scaling could potentially be applied for data augmentation in this context.

CQ: What are the datasets used in the deep learning pipeline (e.g, MNIST, CIFAR, ImageNet)?
Answer: The dataset used in this work has been derived from Planets full-frame analytic scene products using its 4-band satellites in sun-synchronous orbit (SSO) and International Space Station (ISS) orbit. %Query

CQ: What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?
Answer: The preprocessing steps involved before training a deep learning model include resizing the images to a standard size, in this case, 128x128 pixels. This step is important as it helps in speeding up the training. In this dataset, 40479 images for training and 40669 images for testing were used. Each image may be classified into multiple classes. No preprocessing involving the texture and nature of the image itself was performed.

CQ: What is the code repository link of the deep learning pipeline (e.g., Link to GitHub, GitLab, BitBucket)?
Answer: I'm sorry, but the provided context does not include information about the code repository link of the deep learning pipeline.

CQ: What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?
Answer: The proposed work uses the VGG16 model, which is a type of Convolutional Neural Network (CNN).

CQ: What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?
Answer: The Adam optimizer was used to minimize the loss, which is measured by binary cross-entropy, with a learning rate of 10^4. A batch size of 128 was used, and the model was trained for 15 epochs.

CQ: What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?
Answer: The Adam optimizer is applied in the deep learning pipeline. %Explanation Explanation: The Adam optimizer is mentioned in the Methodology section as the optimization technique used to minimize the loss, which is measured by binary cross-entropy, with a learning rate of 10^4.

CQ: What are the regularization methods used to prevent overfitting  in the deep learning pipeline (e.g., dropout, L2 regularization)?
Answer: The paper does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization methods used in deep learning include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which discourages large weights and helps to prevent overfitting.

CQ: Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?
Answer: The helpful answer is that the deep learning model was built using TensorFlow. Final

CQ: Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?
Answer: The deep learning model was trained using an NVIDIA Tesla K80 GPU.

CQ: What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?
Answer: The following metrics were evaluated in our work: Precision, Recall, Accuracy, F-Beta Score, and Categorical Cross Entropy.

CQ: What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?
Answer: The purpose of the deep learning model is to classify images into various classes. %Explanation Explanation: The context states that the VGG16 model has been used to classify images into various classes. %Query

Provide your answer as follows:
        Named Entities: For each provided Concept(Corresponding Named Entity,..), ...