The data augmentation techniques applied in the deep learning pipeline include Flipping, Rotating, Scaling, and Shearing. %Context 34. Krizhevsky, A. ; Sutskever, I. ; Hinton, G. E. ImageNet classiﬁcation with deep convolutional neural networks. Commun. ACM 2017, 60, 84–90. [CrossRef] 35. Huang, Y. Y. ; Wang, W. Deep residual learning for weakly-supervised relation extraction. In Proceedings of the EMNLP 2017—Conference on Empirical Methods in Natural Language Processing, Copenhagen, Denmark, 7–11 September 2017. 36. Tan, M. ; Le, Q. V. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks. arXiv 2019, arXiv:1905. 11946, 37. Huang, G. ; Liu, Z. ; Weinberger, K. Q. Densely Connected Convolutional Networks. arXiv 2016, arXiv:1608. 06993, 38. Wu, S. ; Zhong, S. ; Liu, Y. ResNet. Multimed. Tools Appl. 2017. [CrossRef] 39. Szegedy, C. ; Vanhoucke, V. ; Ioffe, S. ; Shlens, J. ; Wojna, Z. Rethinking the Inception Architecture for Computer Vision. In Proceedings of the 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 26 June–1 July 2016; pp. 2818–2826. 40. R Core Team. R: A Language and Environment for Statistical Computing; R Core Team: Geneva, Switzerland, 2020. Available online: http://softlibre. unizar. es/manuales/aplicaciones/r/fullrefman. pdf (accessed on 9 November 2020). 41. Thomsen, P. F. ; Jørgensen, P. S. ; Bruun, H. H. ; Pedersen, J. ; Riis-Nielsen, T. ; Jonko, K. ; Słowi ´nska, I. ; Rahbek, C. ; Karsholt, O. Resource specialists lead local insect community turnover associated with temperature—Analysis of an 18-year full-seasonal record of moths and beetles. J. Anim. Ecol. 2016, 85, 251–261. [CrossRef] 42. Høye, T. T. ; Ärje, J. ; Bjerge, K. ; Hansen, O. L. P. ; Iosiﬁdis, A. ; Leese, F. ; Mann, H. M. R. ; Meissner, K. ; Melvad, C. ; Raitoharju, J. Deep learning and computer vision will transform entomology. PNAS 2020, in press. [CrossRef] Figure 7. Confusion matrix for the validation of the best model. The numbers corresponds to the species numbers in Table 1. Finally, the customized CNN architectures were compared with selected state-of- the-art CNN optimized architectures. EfﬁcientNetB0 [36] is scaled to work with a small image input size of 224 × 224 pixel and has 4,030,358 learnable parameters. Using the moths dataset with the same data augmentation, the EfﬁcientNetB0 achieved a F1-score of 88. 62%, which is lower than our top ﬁve best architectures. DenceNet121 [37] with 10 of 18 Sensors 2021, 21, 343 7,047,754 learnable parameters gave a F1-score of 84. 93% which is even lower. CNN architectures with many parameters (more than 20,000,000) such as ResNetV50 [38] and InceptionNetV3 [39] gave a high training accuracy, but a lower validation F1-score of 69. 1% and 81. 7%, respectively. This result indicates overﬁtting and that more training data are needed when such large deep learning networks are used. A very high F1-score of 96. 6% was ﬁnally achieved by transfer learning on ResNetV50 using pretrained weights and only training the output layers. This indicates that the state-of-the-art was able to outperform our proposed model, but requires pretrained weights with many more parameters. 2. 4. Summary Statistics The detection of insects was summarized based on the number of counted insects, the number of moth species found, and the number of unknown insects found (i. e. , unknown to the trained CNN algorithm). The statistics were updated as the images were analyzed in order to enable visual inspection of the result during processing. Thus, the statistics were always updated throughout the execution of one iteration of the algorithm see Figure 3. The classiﬁcation phase simultaneously classiﬁed each individual and assigned labels to each individual species. That is, an individual in one image could be classiﬁed as a different species than the same individual in the previous image. This phase ensured that the moth species most frequently classiﬁed in a track was represented in the ﬁnal statistics. An insect was only counted if it was observed in more than three consecutive images thus ignoring noise created by insects ﬂying close to the camera. Several other parameters were deﬁned and adjusted to ﬁlter similar noisy tracks created by very small insects. 3. Experiment The CNN model had four layers for feature detection and two fully connected layers for ﬁnal species classiﬁcation. The optimal architecture was found by using combinations of hyperparameters for the ﬁrst and last layer in the CNN. Below are the parameters used to train different CNN’s for species classiﬁcation. Fixed pool size and stride, n × n, n ∈ {2} Kernel size n × n, n ∈ {1, 3, 5} Convolutional depth n, n ∈ {32, 64, 128} Fully connected size n, n ∈ {256, 512} Optimizer n, n ∈ {Adam, SGD} The optimal chosen CNN architecture is shown in Figure 5. The ﬁrst layer performed convolution using 32 kernels with a kernel size of 5 × 5 followed by maximum pooling of size 2 × 2 and stride 2. All the following layers used a kernel size of 3 × 3. The second and third layer performed convolution using 64 kernels with the same pooling size as mentioned above. The ﬁnal layer also used 64 kernels based on the optimization of • • • • 7 of 18 (5) (6) Sensors 2021, 21, 343 hyperparameters. All convolutional layers used the Rectiﬁed Linear Unit (ReLu) activation function. The last fully connected layer had two hidden layer with 4096 and 512 neurons and a softmax activation function in the output layer. Two of the most commonly used optimizers—Adaptive Moment Estimation (Adam) and Stochastic Gradient Decent (SGD)— were investigated. While Adam was an optimizer that converged relatively quickly, it did so at the expense of a greater loss. SGD, on the other hand, converged more slowly, but achieved a smaller loss. Figure 5. CNN architecture for moth species classiﬁcation used an input of a 128 × 128 RGB image. Developing a deep learning model for classifying species was an iterative process with an alternation between selecting images and training CNN models. When selecting and annotating images, experiments showed that it was important to vary images with different individuals and preferably equal numbers of each species. From the experiment mentioned in Section 2. 3, images of nine different frequently occurring insect species were selected to train the different CNN models. The classes of species were chosen based on the recorded data from where a sufﬁcient number of images could be selected to train the CNN models. According to our screening of the data, no other species were found in sufﬁcient quantity to allow for their inclusion in the dataset. Examples of images of each individual species are shown in Figure 6. To ﬁnd the best CNN architecture for species classiﬁcation, different hyperparameters were adjusted as described in Section 2. A total of 64 architectures were trained using a dropout probability of 0. 3 after the second to last hidden layer. The average F1-score for all classes was used as a measure for a given architecture’s performance. The ﬁve best architectures had high F1-scores, which only varied by 0. 02, but had a varying number of learnable parameters (Table 2). Compared to SGD, Adam turned out to be the superior optimizer for training of all models. In the end, the architecture that had a rating among the three highest F1-score but the lowest amount of learnable parameters (2,197,578) was chosen. The reason for this is that an architecture with many parameters and few training data would increase the risk of overﬁtting the neural network. The chosen model shown in Figure 5 had an F1-score of 92. 75%, which indicated that the trained CNN was very accurate in its predictions. This ﬁnal architecture was chosen because it achieved average precision, recall, and an F1-score of 93%, which indicated a suitable model classiﬁcation. The confusion matrix (Figure 7) was based upon the validation of the chosen model. The confusion matrix has a diagonal trend, which indicates that the model matched the validation set well. The model had a recall of 93%, indicating that only 7% of the moth species in the validation set were missed. A similar precision of 93% was obtained, indicating that only 7% were wrongly classiﬁed. 