The preprocessing steps involved before training a deep learning model include resizing the input images and their annotations to a 512 × 512 resolution to reduce computational cost during training. Rotation, flipping, and brightness adjustments are applied as augmentation techniques for better network generalization. The model is pre-trained on the ImageNet dataset and fine-tuned on the dataset. An Adam optimizer with a learning rate of 1 × 10−4 and a batch size of 3 is used. The model is trained for 100 epochs with a binary cross-entropy loss function. Mask post-processing is also applied to solve the problem of under-segmentation of closely placed leaves. 