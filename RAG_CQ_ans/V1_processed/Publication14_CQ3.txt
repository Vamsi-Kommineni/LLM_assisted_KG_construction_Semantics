The data annotation techniques used in the deep learning pipeline include bounding box annotation and instance segmentation. Bounding box annotation involves drawing a box around an object in an image to identify its location, while instance segmentation involves labeling each pixel in an image to identify the object it belongs to. These techniques are used to create labeled datasets that can be used to train deep learning models. %Context Context: Deng, J. , Dong, W. , Socher, R. , Li, L-J. , Li, K. , & Fei-Fei, L. (2009). ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, (June), (pp. 248–255). https://doi. org/10. 1109/cvprw. 2009. 5206848 Elias, A. R. , Golubovic, N. , Krintz, C. , & Wolski, R. (2017). Where’s the bear? – Automating wildlife image processing using IoT and edge WEINSTEIN 2041210x, 2018, 6, Downloaded from https://besjournals. onlinelibrary. wiley. com/doi/10. 1111/2041-210X. 13011 by Thuringer Universitats- Und, Wiley Online Library on [16/11/2023]. See the Terms and Conditions (https://onlinelibrary. com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License | 1441 Methods in Ecology and Evolu(cid:13)on Wilf, P. , Zhang, S. , Chikkerur, S. , Little, S. A. , Wing, S. L. , & Serre, T. (2016). Computer vision cracks the leaf code. Proceedings of the National Academy of Sciences of the United States of America, 113, 201524473. Zhang, Z. , He, Z. , Cao, G. , & Cao, W. Animal detection from highly cluttered natural scenes using spatiotemporal object region propos- als and patch verification. IEEE Transactions on Multimedia, 18, 2079– 2092. 1109/TMM. 2016. 2594138 How to cite this article: Weinstein BG. Scene- specific convolutional neural networks for video- based biodiversity detection. Methods Ecol Evol. 2018;9:1435–1441. 13011 S U P P O R T I N G I N FO R M AT I O N Additional supporting information may be found online in the Supporting Information section at the end of the article. download for Mac and Windows with the pre- trained humming- 3 | R E S U LT S bird model. In addition, I provide reproducible scripts for local and Google cloud environments to allow users to train new models, Feature extraction of the fixed inception layers completed in 1 hr which can then be used in the local software. and 26 min on 15 CPUs. Training of the new layers completed in 27 min on a single CPU. Model evaluation on the 70 test videos com- 2. 2 | Test dataset pleted in 4 hr and 38 min on 30 CPUs with an average frame rate of 17 frames/s. On average, a video contained 545. 84 candidate mo- My collaborators and I have been studying hummingbird ecol- ogy using time- lapse cameras in the Ecuadorian Andes since 2013 (Weinstein & Graham, 2017). Cameras turn on at dawn, off at dusk, and record a photo every second for up to 5 days. Cameras filming individual flowers capture hummingbirds in less than 1% of images. Hummingbird visits are rapid and rare, lasting 3 to 5 s, with only a handful of visits a day. To train the network, I collected 14,432 image crops containing hummingbirds and 14,432 crops containing back- ground vegetation and sky. To validate the accuracy of the model, I selected 70 half- day videos that represented a range of challeng- ing backgrounds. These videos were not used in the model training. Within these videos there 532 frames (1. 4%) containing humming- birds and 37,677 frames (98. 6%) containing background. Previous analysis showed motion detection to be effective in finding the motion objects based on training data, and can be improved over time with newly labelled data. DeepMeerkat is an extension of MotionMeerkat (Weinstein, 2015), with the addition of a neural network to classify movement objects as either foreground or background. Rather than train a novel ing images based on temporal filtering (Swinnen, Reijniers, Breno, Fingal, & Van Gool, 2016), I used a convolutional neural network (CNN) to classify objects based on their visual features. CNNs are a type of deep learning algorithm that have been successful in image classification tasks (Krizhevsky, Sutskever, & Hinton, 2012). CNNs consist of multiple layers of filters that are applied to the input image to extract features at different levels of abstraction. The final layer of the CNN is a fully connected layer that maps the extracted features to a set of class labels. To train the CNN, I used a dataset of 14,432 images containing hummingbirds and 14,432 images containing background vegetation and sky. The images were collected using time-lapse cameras in the Ecuadorian Andes. The images were cropped to a size of 224 × 224 pixels and labelled as either containing a hummingbird or not. The dataset was split into training and validation sets, with 80% of the images used for training and 20% used for validation. The CNN was trained using the TensorFlow library (Abadi et al. , 2016) on a single NVIDIA Tesla K80 GPU. The network was trained for 50 epochs with a batch size of 32 and a learning rate of 0. 001. The network achieved an accuracy of 98. 6% on the validation set. To evaluate the performance of the CNN on new videos, I selected 70 half-day videos that represented a range of challenging backgrounds. Within these videos there were 532 frames (1. 4%) containing hummingbirds and 37,677 frames (98. The CNN was able to correctly classify 95. 7% of the hummingbird frames and 76. 1% of the background frames using a conservative acceptance threshold of 0. 1. The trade-off between recall and precision varied by acceptance value, with a faster decrease in background recall at low acceptance values (Figure 3). The majority of motion detection objects were well centred and had high foreground classification probabilities (Figure 4). Using the more conservative 0. 1 acceptance value, the remaining misclassified frames (n = 24) had poor bounding box segmentation (Figure 5). Helpful 