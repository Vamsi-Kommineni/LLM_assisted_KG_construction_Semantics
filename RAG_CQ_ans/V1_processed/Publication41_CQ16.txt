Training is considered complete when the training loss has converged. In this case, the model was trained for 15 epochs, by which time the training loss had converged. %Context: haze • partly cloudy • primary • road • selective logging • conventional mine • slash burn • water As a single image can have multiple classes in this dataset so, in the algorithm, all such classes were tried to predict correctly for each of the images. IV. DATA ANALYSIS Some basic data analysis was performed on the dataset which have been described in details below. A. Distribution of Training Labels Firstly, the histogram as present in Figure 4 showing the distribution of training labels was constructed. It has been found that the dataset is not balanced in nature, i. e. , all labels are not present in uniform quantity. Labels such as primary, clear and agriculture are present in signiﬁcantly more number than the other ones. Whereas, some other labels like slash burn, blow down and conventional mine are present in very less quantity. Note that in the dataset, a single image may have multiple classes. The histogram must be seen keeping this in mind. Fig. 4: Distribution of Training Labels B. Correlation Matrix The correlation matrix was plotted, as shown in Figure 5, to understand the occurrence of the classes with respect to each other. Here, redder is the label, more is the value of the correlation for any given pair of classes. After studying this plot, some interesting results were observed. Some of them are: The label primary is associated with almost all classes. This means that most chips have some degree of primary forests along with other labels. The label agriculture is also associated with a few labels like road, habitation and cultivation. 5: Distribution of Training Labels V. PREPROCESSING OF DATASET Even after converting to JPG, the dataset was quite large in size. It would have been computationally expensive to train the model on such a large dataset. Besides, the obtained dataset contained images of various dimensions. Hence, all images were resized to a standard size, in this case, 128x128 pixels. This is also an important step as it helps in speeding up the training. Since the downloaded VGG16 model did not contain the top layer, it was possible to train with images with dimensions (128x128x3) that were different from the dimensions of images used in the original VGG16 model (224x224x3). In this dataset, 40479 images for training and 40669 images for testing were used. Each image may be classiﬁed into multiple classes. VI. METHODOLOGY VI. METHODOLOGY In the proposed work, the VGG16 model has been used to classify images into various classes. Figure 6 shows the original diagram of the VGG model. 6: Distribution of Training Labels In the model, a batch normalization layer was added to the input layer and then fed to the VGG16 model. The last block of the original VGG16 model was removed and the output of the penultimate block of the VGG16 model was ﬂattened. It was then passed on to a softmax classiﬁer to present the output with respect to 17 classes. Here, 20% of the training data was used for validation after training. The architecture of this model is present in Table I. TABLE I: Architecture of VGG16 model Layer (type) input 1 (InputLayer) batch normalization 1 vgg16 (Model) ﬂatten 1 (Flatten) dense 1 (Dense) Output Shape (None, 128, 128, 3) (None, 128, 128, 3) (None, 4, 4, 512) (None, 8192) (None, 17) Parameter 0 12 14714688 0 139281 Here, the Adam optimizer [16] has been used to minimize the loss, which is measured by binary cross- entropy, with a learning rate of 104. Batch size of 128 was used here and this model was trained for 15 epochs. By this time, the training loss had converged. Using an NVIDIA Tesla K80 GPU, this took around one hour to train. The plot between the training loss vs epoch is shown in Figure 7. 7: Plot of Training Loss vs Epoch VII. RESULT The following metrics were evaluated in our work Precision = T P T P + F P Recall = T P T P + F N Accuracy = T P + T N T P + T N + F P + F N With TP, FP, TN,FN being number of true positives, false positives, true negatives and false negatives, respectively. 1 precision + β F-Beta Score = Fβ = 1 β+1 1 recall precision. recall precision + recall 1 β+1 = (1 + β) Categorical Cross Entropy = n (cid:88) K (cid:88) −y(k) truelog(y(k) predict) i k In the experiment, a training loss of 6. 88%, training accu- racy of 97. 35% and testing accuracy of 96. 71% were obtained. Also, an F-beta score of 92. 69% was obtained. The F-beta score is a weighted harmonic mean of the precision and recall. An F-beta score reaches its best value at 1 and worst score at 0. VIII. CONCLUSION VIII. CONCLUSION In this work, a way to classify satellite imagery in an automated manner using deep learning with the help of the VGG16 model has been shown. High accuracy was consuming one hour while training with an NVIDIA Tesla K80 GPU. This model can be successfully applied to track the changing land pattern in the rainforests of Amazon. This data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively. Besides, this model can be used to track natural calamities like ﬂoods, forest ﬁres, etc. IX. FUTURE SCOPE A few additions may be made to this work for improvements mentioned below: Using a larger neural network is likely to give a better result. Models like ResNet and Inception, which are deeper in nature may give better results than the VGG16 model. Also, the dataset may help in better classiﬁcation. In this work, it has been shown how resizing the provided image to 128x128 pixels can be made to obtain good performance. No preprocessing involving the texture and nature of the image itself was performed. increased preprocessing of Performing data augmentation to make the system more robust may be another way of getting better results. Since the satellite images may vary in terms of lighting effect, rotation, shifting, etc. , it may be a good idea to perform data augmentation to enlarge the dataset for better training. These things may be investigated in the upcoming future to improve the accuracy and robustness of this model. REFERENCES [1] Nunes Kehl, Thiago, Viviane Todt, Mauricio Roberto Veronez, and Silvio Csar Cazella, “Amazon rainforest deforestation daily detection tool using artiﬁcial neural networks and satellite images,” Sustainability 4, vol. 10, pp. 2566–2573, 2012. [2] Somnath Rakshit, Suvojit Manna, Sanket Biswas, RiyankaKundu, Priti Gupta, Sayantan Maitra, and Subhas Barman, “Prediction of Diabetes Type-II Using a Two-Class Neural Network,” International Conference on Computational Intelligence, Communications, and Business Analyt- ics,Springer,Singapore, pp. 65–71, 2017. [3] Rowley, Henry A. , Shumeet Baluja, and Takeo Kanade, “Neural network-based face detection,” IEEE Transactions on pattern analysis and machine intelligence, vol. 20, no. 1, pp. 23–28, 1998. [4] Simonyan, Karen, and Andrew Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409. 1556, 2014. [5] He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778. [6] Szegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna, “Rethinking the inception architecture for computer vision,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 2818–2826. [7] Kingma, Diederik P. , and Jimmy Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412. 6980, 2014. [8] Chollet, François, et al. , “Xception: Deep learning with depthwise separable convolutions,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 1800–1807. [9] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger, “Densely connected convolutional networks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700–4708. [10] Howard, Andrew G. , et al. , “MobileNets: Efficient convolutional neural networks for mobile vision applications,” arXiv preprint arXiv:1704. 04861, 2017. [11] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 4510–4520. [12] Tan, Min, and Quoc V. Le, “EfficientNet: Rethinking model scaling for convolutional neural networks,” in Proceedings of the IEEE international conference on computer vision, 2019, pp. 7187–7196. [13] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. Le, “Learning transferable architectures for scalable image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8697–8710. [14] Real, Edgar, et al. , “Regularized evolution for image classifier architecture search,” arXiv preprint arXiv:1802. 01548, 2018. [15] Liu, Hanxiao, et al. , “Progressive neural architecture search,” in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 19–35. [16] Kingma, Diederik P. [17] Chollet, François, et al. [18] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [19] Howard, Andrew G. [20] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [21] Tan, Min, and Quoc V. [22] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [23] Real, Edgar, et al. [24] Liu, Hanxiao, et al. [25] Kingma, Diederik P. [26] Chollet, François, et al. [27] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [28] Howard, Andrew G. [29] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [30] Tan, Min, and Quoc V. [31] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [32] Real, Edgar, et al. [33] Liu, Hanxiao, et al. [34] Kingma, Diederik P. [35] Chollet, François, et al. [36] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [37] Howard, Andrew G. [38] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [39] Tan, Min, and Quoc V. [40] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [41] Real, Edgar, et al. [42] Liu, Hanxiao, et al. [43] Kingma, Diederik P. [44] Chollet, François, et al. [45] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [46] Howard, Andrew G. [47] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [48] Tan, Min, and Quoc V. [49] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [50] Real, Edgar, et al. [51] Liu, Hanxiao, et al. [52] Kingma, Diederik P. [53] Chollet, François, et al. [54] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [55] Howard, Andrew G. [56] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [57] Tan, Min, and Quoc V. [58] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [59] Real, Edgar, et al. [60] Liu, Hanxiao, et al. [61] Kingma, Diederik P. [62] Chollet, François, et al. [63] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [64] Howard, Andrew G. [65] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [66] Tan, Min, and Quoc V. [67] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [68] Real, Edgar, et al. [69] Liu, Hanxiao, et al. [70] Kingma, Diederik P. [71] Chollet, François, et al. [72] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [73] Howard, Andrew G. [74] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [75] Tan, Min, and Quoc V. [76] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [77] Real, Edgar, et al. [78] Liu, Hanxiao, et al. [79] Kingma, Diederik P. [80] Chollet, François, et al. [81] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [82] Howard, Andrew G. [83] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [84] Tan, Min, and Quoc V. [85] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [86] Real, Edgar, et al. [87] Liu, Hanxiao, et al. [88] Kingma, Diederik P. [89] Chollet, François, et al. [90] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [91] Howard, Andrew G. [92] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [93] Tan, Min, and Quoc V. [94] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [95] Real, Edgar, et al. [96] Liu, Hanxiao, et al. [97] Kingma, Diederik P. [98] Chollet, François, et al. [99] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [100] Howard, Andrew G. [101] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [102] Tan, Min, and Quoc V. [103] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [104] Real, Edgar, et al. [105] Liu, Hanxiao, et al. [106] Kingma, Diederik P. [107] Chollet, François, et al. [108] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [109] Howard, Andrew G. [110] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [111] Tan, Min, and Quoc V. [112] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [113] Real, Edgar, et al. [114] Liu, Hanxiao, et al. [115] Kingma, Diederik P. [116] Chollet, François, et al. [117] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [118] Howard, Andrew G. [119] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [120] Tan, Min, and Quoc V. [121] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [122] Real, Edgar, et al. [123] Liu, Hanxiao, et al. [124] Kingma, Diederik P. [125] Chollet, François, et al. [126] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [127] Howard, Andrew G. [128] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [129] Tan, Min, and Quoc V. [130] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [131] Real, Edgar, et al. [132] Liu, Hanxiao, et al. [133] Kingma, Diederik P. [134] Chollet, François, et al. [135] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [136] Howard, Andrew G. [137] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [138] Tan, Min, and Quoc V. [139] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [140] Real, Edgar, et al. [141] Liu, Hanxiao, et al. [142] Kingma, Diederik P. [143] Chollet, François, et al. [144] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [145] Howard, Andrew G. [146] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [147] Tan, Min, and Quoc V. [148] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [149] Real, Edgar, et al. [150] Liu, Hanxiao, et al. [151] Kingma, Diederik P. [152] Chollet, François, et al. [153] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [154] Howard, Andrew G. [155] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [156] Tan, Min, and Quoc V. [157] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [158] Real, Edgar, et al. [159] Liu, Hanxiao, et al. [160] Kingma, Diederik P. [161] Chollet, François, et al. [162] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [163] Howard, Andrew G. [164] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [165] Tan, Min, and Quoc V. [166] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [167] Real, Edgar, et al. [168] Liu, Hanxiao, et al. [169] Kingma, Diederik P. [170] Chollet, François, et al. [171] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [172] Howard, Andrew G. [173] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [174] Tan, Min, and Quoc V. [175] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [176] Real, Edgar, et al. [177] Liu, Hanxiao, et al. [178] Kingma, Diederik P. [179] Chollet, François, et al. [180] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [181] Howard, Andrew G. [182] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [183] Tan, Min, and Quoc V. [184] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [185] Real, Edgar, et al. [186] Liu, Hanxiao, et al. [187] Kingma, Diederik P. [188] Chollet, François, et al. [189] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [190] Howard, Andrew G. [191] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [192] Tan, Min, and Quoc V. [193] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [194] Real, Edgar, et al. [195] Liu, Hanxiao, et al. [196] Kingma, Diederik P. [197] Chollet, François, et al. [198] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [199] Howard, Andrew G. [200] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [201] Tan, Min, and Quoc V. [202] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [203] Real, Edgar, et al. [204] Liu, Hanxiao, et al. [205] Kingma, Diederik P. [206] Chollet, François, et al. [207] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [208] Howard, Andrew G. [209] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [210] Tan, Min, and Quoc V. [211] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [212] Real, Edgar, et al. [213] Liu, Hanxiao, et al. [214] Kingma, Diederik P. [215] Chollet, François, et al. [216] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [217] Howard, Andrew G. [218] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [219] Tan, Min, and Quoc V. [220] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [221] Real, Edgar, et al. [222] Liu, Hanxiao, et al. [223] Kingma, Diederik P. [224] Chollet, François, et al. [225] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [226] Howard, Andrew G. [227] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [228] Tan, Min, and Quoc V. [229] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [230] Real, Edgar, et al. [231] Liu, Hanxiao, et al. [232] Kingma, Diederik P. [233] Chollet, François, et al. [234] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [235] Howard, Andrew G. [236] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [237] Tan, Min, and Quoc V. [238] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [239] Real, Edgar, et al. [240] Liu, Hanxiao, et al. [241] Kingma, Diederik P. [242] Chollet, François, et al. [243] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [244] Howard, Andrew G. [245] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [246] Tan, Min, and Quoc V. [247] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [248] Real, Edgar, et al. [249] Liu, Hanxiao, et al. [250] Kingma, Diederik P. [251] Chollet, François, et al. [252] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [253] Howard, Andrew G. [254] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [255] Tan, Min, and Quoc V. [256] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [257] Real, Edgar, et al. [258] Liu, Hanxiao, et al. [259] Kingma, Diederik P. [260] Chollet, François, et al. [261] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [262] Howard, Andrew G. [263] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [264] Tan, Min, and Quoc V. [265] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [266] Real, Edgar, et al. [267] Liu, Hanxiao, et al. [268] Kingma, Diederik P. [269] Chollet, François, et al. [270] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [271] Howard, Andrew G. [272] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [273] Tan, Min, and Quoc V. [274] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [275] Real, Edgar, et al. [276] Liu, Hanxiao, et al. [277] Kingma, Diederik P. [278] Chollet, François, et al. [279] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [280] Howard, Andrew G. [281] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [282] Tan, Min, and Quoc V. [283] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [284] Real, Edgar, et al. [285] Liu, Hanxiao, et al. [286] Kingma, Diederik P. [287] Chollet, François, et al. [288] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [289] Howard, Andrew G. [290] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [291] Tan, Min, and Quoc V. [292] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [293] Real, Edgar, et al. [294] Liu, Hanxiao, et al. [295] Kingma, Diederik P. [296] Chollet, François, et al. [297] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [298] Howard, Andrew G. [299] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [300] Tan, Min, and Quoc V. [301] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [302] Real, Edgar, et al. [303] Liu, Hanxiao, et al. [304] Kingma, Diederik P. [305] Chollet, François, et al. [306] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [307] Howard, Andrew G. [308] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [309] Tan, Min, and Quoc V. [310] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [311] Real, Edgar, et al. [312] Liu, Hanxiao, et al. [313] Kingma, Diederik P. [314] Chollet, François, et al. [315] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [316] Howard, Andrew G. [317] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [318] Tan, Min, and Quoc V. [319] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [320] Real, Edgar, et al. [321] Liu, Hanxiao, et al. [322] Kingma, Diederik P. [323] Chollet, François, et al. [324] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [325] Howard, Andrew G. [326] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [327] Tan, Min, and Quoc V. [328] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [329] Real, Edgar, et al. [330] Liu, Hanxiao, et al. [331] Kingma, Diederik P. [332] Chollet, François, et al. [333] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [334] Howard, Andrew G. [335] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen, “Mobilenetv2: Inverted residuals and linear bottlenecks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. [336] Tan, Min, and Quoc V. [337] Zoph, Barret, Vijay Vasudevan, Jonathon Shlens, and Quoc V. [338] Real, Edgar, et al. [339] Liu, Hanxiao, et al. [340] Kingma, Diederik P. [341] Chollet, François, et al. [342] Huang, Gao, Zhuang Liu, Laurens van der Maaten, and Kilian Q. [343] Howard, Andrew G. [344] Sandler, Mark, Andrew Howard, Menglong Zhu, Andrey Zhm