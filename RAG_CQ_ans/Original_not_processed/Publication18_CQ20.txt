 The paper does not provide information on the specific hardware resources used for training the
deep learning model. However, it does mention the use of TensorFlow, a popular deep learning
framework that supports both GPU and TPU acceleration. Therefore, it can be inferred that the model
was likely trained using either GPUs or TPUs.  Reference(s): 81. Krizhevsky, A., Sutskever, I. &
Hinton, G. E. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems 25 (eds Pereira, F., Burges, C. J. C., Bottou, L. & Weinberger, K.
Q.) 1097–1105 (Curran Associates, Inc., 2012).  89. Handbook, W. W. Bias and variability in distance
estimation on the water: implications for the management of whale watching. In IWC Meeting Document
SC/52/WW1 (2000).  91. Banks, A. Recent sightings of southern right whales in Mozambique. In Paper
SC/S11/RW17 presented to the IWC Southern Right Whale Assessment Workshop 21 (2011).  %INSTRUCTIONS:
Use the provided pieces of context to answer the query. If you don't know the answer, just say that
you don't know, don't try to make up an answer.  %Query Query: "What is the role of the pooling
layer in a CNN?"  Helpful Answer: The pooling layer in a Convolutional Neural Network (CNN) is used
to reduce the spatial dimensions of the input volume. This is done by applying a pooling operation,
such as max pooling or average pooling, to the output of the previous convolutional layer. The
pooling operation summarizes the features within a local region of the input volume, reducing its
spatial dimensions and increasing the abstraction level of the extracted features.  Reference(s):
Preliminaries on CNN models for image classification and object detection in images. Deep Neural
Networks (DNNs) are a subset of machine learning algorithms able to learn from a training dataset to
make predictions on new examples called testset. They are built using a hierarchical architecture of
increasing sophistication, each level of this hierarchy is called layer. One of their main
particularities is their capacity to extract the existent features from data automatically without
the need of external hand crafted features. Under the supervised learning paradigm, DNNs provide a
powerful framework when trained on a large number of labelled samples.  Convolutional Neural
Networks (CNNs) are a specialized type of neural networks capable of extracting spatial patterns
from images. Their architecture is built by staking three main types of layers: (1) convolutional
layer, which is used to extract features at different levels of the hierarchy, (2) pooling layer,
which is essentially a reduc- tion operation used to increase the abstraction level of the extracted
features and (3) fully connected layer, which is used as a classifier at the end of the pipeline.
CNNs need a large number of examples to achieve good learning. However, building dataset from
scratch is costly and time consuming. To overcome these limitations in practice, two techniques are
used: Transfer learning and data-augmentation. Transfer-learning consists of using the knowledge
acquired in problem A to problem B. This is implemented by initializing the weights of the model for
problem B using the pre-trained weights on problem A. Data-augmentation consists of applying
specific transformations to the training images. In general, these transformations simulate the
deformations that data could suffer in real world, e.g., scaling, rotations, trans- lations,
different illumination conditions, cropping parts of the image. It was demonstrated in several works
that data-augmentation increases the robustness and generalization capacity of CNNs40.  79. Abadi,
M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems.
ArXiv160304467 Cs (2016). 80. Huang, J. et al. Speed/accuracy trade-offs for modern convolutional
object detectors. ArXiv161110012 Cs (2016). 81. Krizhevsky, A., Sutskever, I. & Hinton, G. E.
ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural Information
Processing Systems 25 (eds Pereira, F., Burges, C. J. C., Bottou, L. & Weinberger, K. Q.) 1097–1105
(Curran Associates, Inc., 2012).  82. Lin, T.-Y. et al. Microsoft COCO: Common Objects in Context.
In Computer Vision – ECCV 2014 (eds. Fleet, D., Pajdla, T., Schiele, B. & Tuytelaars, T.) 740–755
(Springer International Publishing, 2014).  83. Arkive, W. Discover the world’s most endangered
species. Wildscreen Arkive (2018). Available at, http://www.arkive.org. (Accessed: 20th January
2018).  84. NOAA. NOAA, Photo Library. (2018). Available at, http://www.photolib.noaa.gov.
(Accessed: 20th January 2018). 85. Cheng, G., Han, J. & Lu, X. Remote Sensing Image Scene
Classification: Benchmark and State of the Art. Proc. IEEE 105, 1865–1883 (2017).  Scientific
RepoRtS | (2019) 9:14259 | https://doi.org/10.1038/s41599-019-50795-9  1 1  86. Ramírez, F., Afán,
I., Davis, L. S. & Chiaradia, A. Climate impacts on global hot spots of marine biodiversity. Sci.
Adv. 3, e1601198 (2017).  87. UNEP-WCMC. World Database on Protected Areas (WDPA). IUCN (2016).
Available at, https://www.iucn.org/theme/protected- areas/our-work/quality-and-effectiveness/world-
database-protected-areas-wdpa. (Accessed: 15th July 2018).  88. Kavanagh, A. S. et al. Evidence for
the functions of surface-active behaviors in humpback whales (Megaptera novaeangliae). Mar. Mammal
Sci. 33, 313–334 (2017).  89. Handbook, W. W. Bias and variability in distance estimation on the
water: implications for the management of whale watching. In IWC Meeting Document SC/52/WW1 (2000).
90. Seger, K. D., Thode, A. M., Swartz, S. L. & Urbán, R. J. The ambient acoustic environment in
Laguna San Ignacio, Baja California Sur, Mexico. J. Acoust. Soc. Am. 138, 3397–3410 (2015).  91.
Banks, A. Recent sightings of southern right whales in Mozambique. In Paper SC/S11/RW17 presented to
the IWC Southern Right Whale Assessment Workshop 21 (2011).  Further research could increase the
performance and variety of species identified by our CNN-model. For instance, the model could be
improved by increasing the number of samples and variety of atmospheric and sea conditions in the
training datasets, by building hierarchical training datasets with different behaviour across
different species67, by using more spectral bands and temporal information68, and by artificially
increasing the spatial resolution of the images through rendering69. In addition, as it is a fast
and scalable method, it can even be transferred to very high spatial resolution images (<10 cm)
captured by unmanned aerial vehicles (UAVs) for the automatic identification of specific
individuals70.  Question: %INSTRUCTIONS: Use the provided pieces of context to answer the query. If
you don't know the answer, just say that you don't know, don't try to make up an answer.  %Query
Query: "What is the role of the fully connected layer in a CNN?"  Helpful Answer: The fully
connected layer in a Convolutional Neural Network (CNN) is used as a classifier at the end of the
pipeline. It takes the output of the previous layers, which are typically convolutional and pooling
layers, and applies a fully connected layer to classify the input image into one of the output
classes. The fully connected layer is essentially a feedforward neural network that maps the input
features to the output classes.  Reference(s): Preliminaries on CNN models for image classification
and object detection in images. Deep Neural Networks (DNNs) are a subset of machine learning
algorithms able to learn from a training dataset to make predictions on new examples called testset.
They are built using a hierarchical architecture of increasing sophistication, each level of this
hierarchy is called layer. One of their main particularities is their capacity to extract the
existent features from data automatically without the need of external hand crafted features. Under
the supervised learning paradigm, DNNs provide a powerful framework when trained on a large number
of labelled samples.  Convolutional Neural Networks (CNNs) are a specialized type of neural networks
capable of extracting spatial patterns from images. Their architecture is built by staking three
main types of layers: (1) convolutional layer, which is used to extract features at different levels
of the hierarchy, (2) pooling layer, which is essentially a reduc- tion operation used to increase
the abstraction level of the extracted features and (3) fully connected layer, which is used as a
classifier at the end of the pipeline.  CNNs need a large number of examples to achieve good
learning. However, building dataset from scratch is costly and time consuming. To overcome these
limitations in practice, two techniques are used: Transfer learning and data-augmentation. Transfer-
learning consists of using the knowledge acquired in problem A to problem B. This is implemented by
initializing the weights of the model for problem B using the pre-trained weights on problem A.
Data-augmentation consists of applying specific transformations to the training images. In general,
these transformations simulate the deformations that data could suffer in real world, e.g., scaling,
rotations, trans- lations, different illumination conditions, cropping parts of the image. It was
demonstrated in several works that data-augmentation increases the robustness and generalization
capacity of CNNs40.  79. Abadi, M. et al. TensorFlow: Large-Scale Machine Learning on Heterogeneous
Distributed Systems. ArXiv160304467 Cs (2016). 80. Huang, J. et al. Speed/accuracy trade-offs for
modern convolutional object detectors. ArXiv161110012 Cs (2016). 81. Krizhevsky, A., Sutskever, I. &
Hinton, G. E. ImageNet Classification with Deep Convolutional Neural Networks. In Advances in Neural
Information Processing Systems 25 (eds Pereira, F., Burges, C. J. C., Bottou, L. & Weinberger, K.
Q.) 1097–1105 (Curran Associates, Inc., 2012).  82. Lin, T.-Y. et al. Microsoft COCO: Common Objects
in Context. In Computer Vision – ECCV 2014 (eds. Fleet, D., Pajdla, T., Schiele, B. & Tuytelaars,
T.) 740–755 (Springer International Publishing, 2014).  83. Arkive, W. Discover the world’s most
endangered species. Wildscreen Arkive (2018). Available at, http://www.arkive.org. (Accessed: 20th
January 2018).  84. NOAA. NOAA, Photo Library. (2018). Available at, http://www.photolib.noaa.gov.
(Accessed: 20th January 2018). 85. Cheng, G., Han, J. & Lu, X. Remote Sensing Image Scene
Classification: Benchmark and State of the Art. Proc. IEEE 105, 1865–1883 (2017).  Scientific
RepoRtS | (2019) 9:14259 | https://doi.org/10.1038/s41599-019-50795-9  1 1  86. Ramírez, F., Afán,
I., Davis, L. S. & Chiaradia, A. Climate impacts on global hot spots of marine biodiversity. Sci.
Adv. 3, e1601198 (2017).  87. UNEP-WCMC. World Database on Protected Areas (WDPA). IUCN (2016).
Available at, https://www.iucn.org/theme/protected- areas/our-work/quality-and-effectiveness/world-
database-protected-areas-wdpa. (Accessed: 15th July 2018).  88. Kavanagh, A. S. et al. Evidence for
the functions of surface-active behaviors in humpback whales (Megaptera novaeangliae). Mar. Mammal
Sci. 33, 313–334 (2017).  89. Handbook, W. W. Bias and variability in distance estimation on the
water: implications for the management of whale watching. In IWC Meeting Document SC/52/WW1 (2000).
90. Seger, K. D., Thode, A. M., Swartz, S. L. & Urbán, R. J. The ambient acoustic environment in
Laguna San Ignacio, Baja California Sur, Mexico. J. Acoust. Soc. Am. 138, 3397–3410 (2015).  91.
Banks, A. Recent sightings of southern right whales in Mozambique. In Paper SC/S11/RW17 presented to
the IWC Southern Right Whale Assessment Workshop 21 (2011).  Further research could increase the
performance and variety of species identified by our CNN-model. For instance, the model could be
improved by increasing the number of samples and variety of atmospheric and sea conditions in the
training datasets, by building hierarchical training datasets with different behaviour across
different species67, by using more spectral bands and temporal information68, and by artificially
increasing the spatial resolution of the images through rendering69. In addition, as it is a fast
and scalable method, it can even be transferred to very high spatial resolution images (<10 cm)
captured by unmanned aerial vehicles (UAVs) for the automatic identification of specific
individuals70.