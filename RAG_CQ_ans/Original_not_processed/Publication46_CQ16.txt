 The performance evaluation of the model is conducted by tuning the learnable parameters, kernels,
and weight by a loss function through the forward propagation followed by updating these parameter
values through an optimization algorithm either by backpropagation or gradient descent. In any type
of modeling, performance evaluation is an essential task. In terms of validation of species
probability distribution, the AUC (Area Under ROC (Receiver Operating Characteristics) Curve) is one
of the most used performance evaluation metrics. The primary application of the ROC curve is in the
threshold independent assessment that characterizes the model performance at various discrimination
thresholds. This application was found in raster-based studies focusing on predicting land use and
land cover, species distribution modeling, risk assessment, and other probability mappings. The AUC
is generated by plotting the True Positive Rate (TPR) versus the False Positive Rate (FPR) at varied
thresholds. The TPR is also known as sensitivity, probability of detection, or recall, and the FPR
is also known as the probability of false alarm. Therefore, an accurate model will generate a ROC
curve away from the 1:1 line, and a less accurate model will have a ROC curve towards the 1:1 line.
The range of the AUC varies from 0 to 1. The closer the value is to 1, the better the prediction is.
The plots can be described mathematically as:  TPR or Sensitivity or Recall or Probability of
Detection =  TP TP + FN  × 100  Specificity =  TN TN + FP  × 100  FPR or Probability of false alarm
= 1 − Specificity  Here, TP stands for true positive, and FP is false positive, where specificity is
also termed the true negative rate. The TPR provides the percentage of correctly predicted instances
of species other than rhododendron, whereas specificity provides the percentage of correctly
predicted instances of rhododendron distribution.  Answer: The criteria used to determine when
training is complete is the performance evaluation of the model, which is conducted by tuning the
learnable parameters, kernels, and weight by a loss function through the forward propagation
followed by updating these parameter values through an optimization algorithm either by
backpropagation or gradient descent. In terms of validation of species probability distribution, the
AUC (Area Under ROC (Receiver Operating Characteristics) Curve) is one of the most used performance
evaluation metrics. The AUC is generated by plotting the True Positive Rate (TPR) versus the False
Positive Rate (FPR) at varied thresholds. The TPR is also known as sensitivity, probability of
detection, or recall, and the FPR is also known as the probability of false alarm. The range of the
AUC varies from 0 to 1. The closer the value is to 1, the better the prediction is. The plots can be
described mathematically as:  TPR or Sensitivity or Recall or Probability of Detection =  TP TP + FN
× 100  Specificity =  TN TN + FP  × 100  FPR or Probability of false alarm = 1 − Specificity  Here,
TP stands for true positive, and FP is false positive, where specificity is also termed the true
negative rate. The TPR provides the percentage of correctly predicted instances of species other
than rhododendron, whereas specificity provides the percentage of correctly predicted instances of
rhododendron distribution.