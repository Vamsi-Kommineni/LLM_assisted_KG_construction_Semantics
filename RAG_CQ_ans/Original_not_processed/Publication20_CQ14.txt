 The hyperparameters of the model are optimized by using classic hyper-parameters for a fast
convergence of the network without over-fitting. These include a learning rate of 10, an exponential
learning decay with a Gamma of 0.95, a dropout of 50%, and an Adam Solver type. The training lasted
8 days on a computer with 64GB of RAM, an i7 3.50GHz CPU, and a Titan X GPU card for 900,000 images.
The network was trained and run on Caï¬€e (Jia et al., 2014).