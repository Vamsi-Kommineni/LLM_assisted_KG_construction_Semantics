 The strategy implemented to monitor the model performance during training is to stop the network
training after 70 epochs to prevent overfitting. The learning rate used was 10 with a learning decay
of Gamma 0.95, a dropout of 50%, and an Adam Solver type as learning parameters. These are classic
hyperparameters for fast convergence of the network without overfitting. The training lasted 8 days
on a computer with 64GB of RAM, an i7 3.50GHz CPU, and a Titan X GPU card for 900,000 images.