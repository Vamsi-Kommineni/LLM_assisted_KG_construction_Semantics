CQ_no,CQ,CQ_ans,Human_Evaluation
2,"What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?",The dataset for this work has been derived from Planets full-frame analytic scene products using its 4-band satellites in sun-synchronous orbit (SSO) and International Space Station (ISS) orbit. The data format used in the deep learning pipeline is Chip (Image) Data Format.,TRUE
5,"What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? ",The dataset used in this work has been derived from Planets full-frame analytic scene products using its 4-band satellites in sun-synchronous orbit (SSO) and International Space Station (ISS) orbit. %Query,TRUE
6,"What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?","The preprocessing steps involved before training a deep learning model include resizing the images to a standard size, in this case, 128x128 pixels. This step is important as it helps in speeding up the training. In this dataset, 40479 images for training and 40669 images for testing were used. Each image may be classified into multiple classes. No preprocessing involving the texture and nature of the image itself was performed. ",TRUE
4,"What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?","The paper does not provide specific details on the data augmentation techniques applied in the deep learning pipeline. However, it does mention that data augmentation could be performed to make the system more robust, considering that satellite images may vary in terms of lighting effect, rotation, shifting, etc. Therefore, it can be inferred that techniques such as flipping, rotation, and scaling could potentially be applied for data augmentation in this context. ",TRUE
12,"What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?","The proposed work uses the VGG16 model, which is a type of Convolutional Neural Network (CNN). ",TRUE
15,"What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?","The Adam optimizer is applied in the deep learning pipeline. %Explanation Explanation: The Adam optimizer is mentioned in the Methodology section as the optimization technique used to minimize the loss, which is measured by binary cross-entropy, with a learning rate of 10^4. ",TRUE
13,"What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?","The Adam optimizer was used to minimize the loss, which is measured by binary cross-entropy, with a learning rate of 10^4. A batch size of 128 was used, and the model was trained for 15 epochs. ",TRUE
22,"What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?","The following metrics were evaluated in our work: Precision, Recall, Accuracy, F-Beta Score, and Categorical Cross Entropy. ",TRUE
17,"What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?","The paper does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline. However, common regularization methods used in deep learning include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to make the model more robust and prevents overfitting. L2 regularization, also known as weight decay, is a technique that adds a penalty term to the loss function, which discourages large weights and helps to prevent overfitting. ",TRUE
19,"Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?",The helpful answer is that the deep learning model was built using TensorFlow. Final,TRUE
20,"Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?",The deep learning model was trained using an NVIDIA Tesla K80 GPU.,TRUE
8,"Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?",The code repository of the deep learning pipeline is not mentioned in the provided context.,TRUE
25,"What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?",The purpose of the deep learning model is to classify images into various classes. %Explanation Explanation: The context states that the VGG16 model has been used to classify images into various classes. %Query,TRUE
