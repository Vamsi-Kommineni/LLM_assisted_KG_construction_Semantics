CQ_no,CQ,CQ_ans,Human_Evaluation
2,"What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?","The deep learning pipeline uses audio data. The audio data is converted to a Mel spectrogram representation with 32 frequency bins, represented as rows in the spectrogram, using a temporal resolution of 21 columns per second of raw audio. Each column in the spectrogram is computed by running the fast Fourier transform on a section of the audio time signal. Each spectrogram column is computed from 0. 0928 s of audio (which corresponds to a window size of 2,048 samples), and has a Hann window applied. The columns are extracted from the audio signal at a frequency of 21. 53 Hz (or equivalently with a hop length of 1,024 audio samples, on our 22,050 Hz audio). Before use in the classifier, the values of the spectrogram S are converted to a log-scale representation, using the formula ln(A + B * S). For CityBioNet the parameters A = 0. 001 and B = 10. 0 are used, while for CityAnthroNet the parameters A = 0. 025 and B = 2. 0 are used. These parameters are chosen manually to emphasise biotic and anthropogenic sounds by visually inspecting the transformed spectrograms. ",TRUE
5,"What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? ","The deep learning pipeline uses the CitySounds2017 dataset, an expert-annotated dataset of urban sounds collected across Greater London, UK. This dataset was used to train the CNNs, CityBioNet and CityAnthroNet, for measuring audible (0-12 kHz) biotic and anthropogenic acoustic activity in audio recordings from urban environments. ",TRUE
6,"What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?","The preprocessing steps involved before training a deep learning model include normalization, scaling, and cleaning. Normalization is a technique often applied as part of data preparation for machine learning. It is a process that changes the values of numeric columns in the dataset to a common scale, without distorting differences in the ranges of values or losing information. Scaling is a feature scaling technique in which values are rescaled so that they end up ranging between 0 and 1. Cleaning is the process of removing or correcting erroneous data from a dataset. ",Generalized
4,"What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?","The context does not provide information on the specific data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide a helpful answer to this query. ",TRUE
12,"What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?",The pipeline uses a Convolutional Neural Network (CNN) model.,TRUE
15,"What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?","The helpful answer is that the optimization technique applied in the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, it is not possible to determine which optimization technique was used. %Query ",TRUE
13,"What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?","The helpful answer is that the hyperparameters used in the deep learning model are not explicitly mentioned in the provided context. Therefore, it is not possible to provide a specific answer to this query based on the given information. Final ",TRUE
22,"What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?","The performance of the deep learning model is evaluated using precision and recall metrics. The fraction of true positives, false positives, true negatives, and false negatives were also computed, using the same threshold. These analyses were conducted in python v. 2. 7. 12 using scikit-learn v. 0. 18. 1 and matplotlib v. 1. 5. The experiments were run on a machine running Ubuntu 16. 04 with a 3. 60 GHz Xeon processor, 64 GB of RAM, and a 2 GB Nvidia GPU. ",TRUE
17,"What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?","The context does not provide information on the regularization methods used to prevent overfitting in the deep learning pipeline. Therefore, I don't know the answer to this query. ",TRUE
19,"Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?","The deep learning model is built using Theano v. 0. 9. 0 and lasagne v. 2 for machine learning, and librosa v. 4. 2 for audio processing. ",TRUE
20,"Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?","The context does not provide information on the specific hardware resources used for training the deep learning model. Therefore, I cannot provide an answer to this query. ",FALSE
8,"Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?","The code repository of the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, I cannot provide the exact location of the code repository. ",FALSE
25,"What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?","The purpose of the deep learning model is to generate measures of biotic and anthropogenic acoustic activity. The model is a convolutional neural network (CNN) and is part of the CityNet system. The CNN is trained on a large dataset of annotated audio recordings collected across Greater London, UK. The CNN is used to predict the presence or absence of biotic or anthropogenic acoustic activity in each 1-second chunk of audio. The predictions can then be aggregated to give summaries over time or space. ",TRUE
