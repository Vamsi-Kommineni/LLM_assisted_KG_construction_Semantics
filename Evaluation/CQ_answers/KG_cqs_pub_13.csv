CQ_no,CQ,CQ_ans,Human_Evaluation
2,"What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?",The data formats used in the deep learning pipeline are images.,TRUE
5,"What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? ","The datasets used in the deep learning pipeline are Local High Quality (LH), Local Low Quality (LL), and Global High Quality (GH). ",TRUE
6,"What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?",The context does not provide information on the preprocessing steps involved before training a deep learning model.,FALSE
4,"What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?",The context does not provide information on the specific data augmentation techniques applied in the deep learning pipeline.,TRUE
12,"What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?",The pipeline uses a convolutional neural network (CNN) model.,TRUE
15,"What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?",The context does not provide information on the specific optimization techniques applied in the deep learning pipeline.,TRUE
13,"What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?","The hyperparameters used in the deep learning model are not explicitly mentioned in the provided context. However, it is mentioned that the number of units in the two FC layers (512 and 256 for the first and second FC layers respectively) and the dropout rate were determined by five-fold cross- validation with a random 200 images of the GH dataset. These hyperparameters were used throughout all classification tasks in this study. ",TRUE
22,"What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?","The performance of the deep learning model is evaluated using the following metrics: accuracy, multiclass recall rate, multiclass precision, and the F1-score. The accuracy is measured as the proportion of successful predictions in the test set. The multiclass recall rate is defined as a proportion of correct predictions of a class out of the actual number of images of that class. The multiclass precision is defined as a proportion of correct predictions of a class out of the number of images predicted as that class. The F1-score is a harmonic mean of the multiclass recall rate and precision. ",TRUE
17,"What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?","The dropout rate was determined by five-fold cross-validation with a random 200 images of the GH dataset, and these hyperparameters were used throughout all classification tasks in this study. ",TRUE
19,"Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?",The deep learning model is built using TensorFlow and Keras.,TRUE
20,"Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?",The context does not provide information on the hardware resources used for training the deep learning model.,TRUE
8,"Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?","The code repository of the deep learning pipeline is not explicitly mentioned in the provided context. Therefore, I cannot provide the exact location of the code repository. ",TRUE
25,"What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?","The purpose of the deep learning model is for family-level classification of below-ground bulk samples of Coleoptera from understudied forests of Cyprus. The model is trained with images from local surveys versus global databases of above-ground samples from tropical forests and evaluated how prediction accuracy was affected by the quality and resolution of images, the size and complexity of the training set, and the transferability of identifications across very disparate source-target pairs that do not share any species or genera. The within-dataset classification accuracy reached 98% and depended on the number and quality of training images and on dataset complexity. The accuracy of between-datasets predictions was reduced to a maximum of 82% and depended greatly on the standardisation of the imaging procedure. When the source and target images were of similar quality and resolution, albeit from different faunas, the reduction of accuracy was minimal. Application of algorithms for domain adaptation significantly improved the prediction performance of models trained by non-standardised, low-quality images. The findings demonstrate that existing databases can be used to train models and successfully classify images from unexplored biota, when the imaging conditions and classification algorithms are carefully considered. Also, the results provide guidelines for data acquisition and algorithmic development for high-throughput image-based biodiversity surveys. ",TRUE
