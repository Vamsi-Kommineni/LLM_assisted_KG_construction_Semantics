CQ_no,CQ,CQ_ans,Human_Evaluation
2,"What data formats are used in the deep learning pipeline (e.g., image, audio, video, CSV)?","The context does not provide information on the specific data formats used in the deep learning pipeline. However, it mentions that the Convolutional Neural Network (CNN) is used for processing multidimensional data that follows a grid pattern. This suggests that the data format could be an image or a similar grid-like data structure. ",TRUE
5,"What are the datasets used in the deep learning pipeline (e.g., MNIST, CIFAR, ImageNet)? ",The context does not provide information on the specific datasets used in the deep learning pipeline.,TRUE
6,"What preprocessing steps are involved before training a deep learning model (e.g., normalization, scaling, cleaning)?","The context does not provide information on the preprocessing steps involved before training a deep learning model. However, common preprocessing steps include normalization, scaling, and cleaning the data. Normalization is the process of rescaling the data to a specific range, often between 0 and 1. Scaling is the process of adjusting the range of the data to a specific scale, often between -1 and 1. Cleaning the data involves removing any missing or irrelevant data points. %Query ",TRUE
4,"What are the data augmentation techniques applied in the deep learning pipeline (e.g., Fliping, Roating, Scaling)?","The context does not provide information on the data augmentation techniques applied in the deep learning pipeline. Therefore, I cannot provide an answer to this query. ",TRUE
12,"What type of deep learning model is used in the pipeline (e.g., CNN, RNN, Transformer)?",The pipeline uses a Convolutional Neural Network (CNN) as the deep learning model.,TRUE
15,"What optimization techniques are applied in the deep learning pipeline (e.g., SGD, Adam)?","The performance evaluation of the model is conducted by tuning the learnable parameters, kernels, and weight by a loss function through the forward propagation followed by updating these parameter values through an optimization algorithm either by backpropagation or gradient descent. The answer does not explicitly mention the optimization techniques applied in the deep learning pipeline, but it does mention that the parameter values are updated through an optimization algorithm, either by backpropagation or gradient descent. However, it does not specify which optimization techniques, such as SGD or Adam, are used. Therefore, the answer is not fully helpful in this context. Final ",TRUE
13,"What are the hyperparameters used in the deep learning model (e.g., learning rate, optimizer)?","The provided context does not explicitly mention the hyperparameters used in the deep learning model, such as learning rate or optimizer. However, it does mention that the hyperparameters involved in convolution operations are the size and number of kernels. The size could be anything from 3 × 3 to 5 × 5 to 7 × 7, and the kernel could be chosen randomly. ",TRUE
22,"What metrics are used to evaluate the performance of the deep learning model (e.g., accuracy, precision, recall)?","The performance evaluation of the deep learning model is conducted using the Area Under ROC (Receiver Operating Characteristics) Curve (AUC) and Cohen's kappa. The AUC is generated by plotting the True Positive Rate (TPR) versus the False Positive Rate (FPR) at varied thresholds. The TPR is also known as sensitivity, probability of detection, or recall, and the FPR is also known as the probability of false alarm. An accurate model will generate a ROC curve away from the 1:1 line, and a less accurate model will have a ROC curve towards the 1:1 line. The range of the AUC varies from 0 to 1. The closer the value is to 1, the better the prediction is. Cohen’s kappa is also calculated to support the AUC value. Being one of the most popular performance evaluation indices, it is considered to be less complex and dependent on prevalence. The kappa value ranges from −1 to +1, where +1 indicates the perfect agreement. Other than kappa, the True Skill Statistic (TSS) is also incorporated, as it corrects the unimodel dependency of kappa. TSS is widely used in ecology, and it can be explained as TSS = Sensitivity − Speciﬁcity − 1. ",TRUE
17,"What are the regularization methods used to prevent overfitting in the deep learning pipeline (e.g., dropout, L2 regularization)?","The provided context does not mention any specific regularization methods used to prevent overfitting in the deep learning pipeline. However, some common regularization methods used in deep learning include dropout and L2 regularization. Dropout is a technique where randomly selected neurons are ignored during training, which helps to prevent overfitting by reducing the model's reliance on any single neuron. L2 regularization, also known as weight decay, adds a penalty term to the loss function that discourages large weights, which can help to prevent overfitting by reducing the model's complexity. ",TRUE
19,"Which frameworks are used to build the deep learning model (e.g., TensorFlow, PyTorch)?",The context does not provide information on the specific frameworks used to build the deep learning model.,TRUE
20,"Which hardware resources are used for training the deep learning model (e.g., GPUs, TPUs)?",The context does not provide information on the hardware resources used for training the deep learning model.,TRUE
8,"Where is the code repository of the deep learning pipeline available (e.g., GitHub, GitLab, BitBucket)?",The code repository of the deep learning pipeline is not mentioned in the provided context.,TRUE
25,"What is the purpose of the deep learning model (e.g., classification, segmentation, detection)?","The purpose of the deep learning model in this study is to use the Convolutional Neural Network (CNN) architecture for Species Distribution Modelling (SDM). The model is developed in such a way that the algorithm learns and adapts to the spatial hierarchies of features by itself from the lower to the higher levels of the pattern. The model is composed of three layers or building blocks: convolution, pooling, and fully connected layers. Feature extraction is conducted using the first two layers and mapping the extracted features to the output is conducted by the third layer. The model is used to track and map the distribution of the Rhododendron arboreum species based on its occurrence and several ecological and bioclimatic satellite-based observations. The CNN based probability distribution model outperformed the presence only based BIOCLIM model with an AUC score of 0. 917. ",TRUE
